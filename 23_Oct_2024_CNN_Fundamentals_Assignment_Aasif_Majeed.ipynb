{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a5bad7",
   "metadata": {},
   "source": [
    "# CNN Fundamentals — Assignment  \n",
    "**Submitter Name:** Aasif Majeed  \n",
    "**Date:** 23 Oct 2024  \n",
    "\n",
    "This notebook answers all questions from the **CNN Fundamentals** assignment.  \n",
    "It includes:\n",
    "- Each question written with its number (as in the PDF)\n",
    "- Detailed explanations with intuitive examples\n",
    "- Small NumPy demos for image representation, convolution, padding/stride output size, and pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e5ca4",
   "metadata": {},
   "source": [
    "## Q1) Explain the basic components of a digital image and how it is represented in a computer. State the differences between grayscale and color images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b2ea0",
   "metadata": {},
   "source": [
    "\n",
    "### What is a digital image?\n",
    "A **digital image** is a discrete (sampled) representation of a continuous scene. Computers store images as **arrays (matrices)** of numbers.  \n",
    "Each number describes the **intensity** (brightness) or **color** at a specific location called a **pixel**.\n",
    "\n",
    "### Basic components of a digital image\n",
    "1. **Pixels (picture elements)**  \n",
    "   - The smallest addressable unit in an image grid.\n",
    "   - Each pixel has a value that represents intensity (grayscale) or color (RGB).\n",
    "\n",
    "2. **Spatial resolution (width × height)**  \n",
    "   - Example: 256×256 image has 65,536 pixels.\n",
    "   - Higher resolution → more detail (but more memory).\n",
    "\n",
    "3. **Channels (depth)**  \n",
    "   - Grayscale: 1 channel (intensity).\n",
    "   - Color: usually 3 channels (Red, Green, Blue).  \n",
    "   - Some images include **alpha** (transparency), giving 4 channels (RGBA).\n",
    "\n",
    "4. **Bit depth (how many values per pixel)**  \n",
    "   - 8-bit grayscale: pixel values in [0, 255].  \n",
    "   - 16-bit images allow a wider range.\n",
    "   - Higher bit depth → smoother gradients, more dynamic range.\n",
    "\n",
    "### Representation in a computer\n",
    "An image is stored as a NumPy-like array:\n",
    "\n",
    "- **Grayscale image**: shape = `(H, W)`  \n",
    "  Example: `img[10, 20] = 128` means pixel at row 10 col 20 has mid-level intensity.\n",
    "\n",
    "- **Color (RGB) image**: shape = `(H, W, 3)`  \n",
    "  Example: `img[10, 20] = [255, 0, 0]` → bright red at that pixel.\n",
    "\n",
    "### Grayscale vs Color: key differences\n",
    "| Aspect | Grayscale | Color (RGB) |\n",
    "|---|---|---|\n",
    "| Channels | 1 | 3 (R,G,B) |\n",
    "| Array shape | (H, W) | (H, W, 3) |\n",
    "| Pixel meaning | intensity only | mixture of red/green/blue |\n",
    "| Memory | lower | ~3× more than grayscale |\n",
    "| Use cases | medical scans, edge detection, some CV preprocessing | natural images, object recognition, segmentation |\n",
    "\n",
    "**Intuition:** grayscale tells “how bright”; color tells “how bright in red, green, and blue components.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7adba",
   "metadata": {},
   "source": [
    "## Q2) Define Convolutional Neural Networks (CNNs) and discuss their role in image processing. Describe the key advantages of using CNNs over traditional neural networks for image-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929c992",
   "metadata": {},
   "source": [
    "\n",
    "### What is a CNN?\n",
    "A **Convolutional Neural Network (CNN)** is a type of neural network designed specifically to process **grid-like data**, especially **images**.  \n",
    "CNNs use **convolutional layers** (learnable filters/kernels) to automatically learn spatial features such as:\n",
    "- edges\n",
    "- textures\n",
    "- corners\n",
    "- shapes\n",
    "- object parts\n",
    "\n",
    "### Role in image processing\n",
    "CNNs are used in:\n",
    "- **Image classification** (cat vs dog, digit recognition)\n",
    "- **Object detection** (finding bounding boxes)\n",
    "- **Segmentation** (pixel-level labeling)\n",
    "- **Face recognition**\n",
    "- **Medical imaging** (tumor detection)\n",
    "- **Super-resolution / denoising**\n",
    "\n",
    "CNNs learn features directly from data, replacing manual feature engineering (e.g., SIFT/HOG in classic computer vision).\n",
    "\n",
    "### Why CNNs are better than traditional (fully-connected) neural networks for images\n",
    "A traditional dense neural network (MLP) treats the input as a flat vector and loses spatial structure. CNNs keep and exploit the 2D/3D structure.\n",
    "\n",
    "**Key advantages:**\n",
    "1. **Local connectivity**\n",
    "   - A filter looks at a small neighborhood (e.g., 3×3), matching the fact that nearby pixels are strongly related.\n",
    "\n",
    "2. **Parameter sharing (same filter used across the image)**\n",
    "   - Greatly reduces parameters.\n",
    "   - A 3×3 filter has only 9 weights (per channel), regardless of image size.\n",
    "\n",
    "3. **Translation equivariance**\n",
    "   - If an object shifts, convolution responses shift similarly.\n",
    "   - Helps detect patterns anywhere in the image.\n",
    "\n",
    "4. **Hierarchical feature learning**\n",
    "   - Early layers learn edges, middle layers learn textures/parts, deeper layers learn object-level concepts.\n",
    "\n",
    "5. **Better generalization & efficiency**\n",
    "   - Fewer parameters → less overfitting and faster training compared to dense networks on images.\n",
    "\n",
    "**Example:**  \n",
    "For a 64×64×3 image:\n",
    "- Flattened input = 12,288 features. A dense layer with 1,000 neurons would need ~12 million weights.\n",
    "- CNN filters reuse weights and need far fewer parameters to learn meaningful visual features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303ca05",
   "metadata": {},
   "source": [
    "## Q3) Define convolutional layers and their purpose in a CNN. Discuss the concept of filters and how they are applied during the convolution operation. Explain the use of padding and strides in convolutional layers and their impact on the output size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc85e56",
   "metadata": {},
   "source": [
    "\n",
    "### Convolutional layer (Conv layer)\n",
    "A **convolutional layer** applies multiple small learnable matrices (**filters/kernels**) to an input image/feature map to produce new **feature maps**.\n",
    "\n",
    "**Purpose:**\n",
    "- Extract spatial features like edges and textures.\n",
    "- Build more complex features across layers.\n",
    "\n",
    "### Filters (kernels) and convolution operation\n",
    "A **filter** is a small matrix, e.g., 3×3, 5×5.\n",
    "The filter slides across the image, computing a dot product at each position:\n",
    "\n",
    "\\[\n",
    "\\text{output}(i,j) = \\sum_{u}\\sum_{v} \\text{input}(i+u, j+v)\\cdot \\text{kernel}(u,v)\n",
    "\\]\n",
    "\n",
    "- Each filter learns to detect a specific pattern (horizontal edges, vertical edges, texture, etc.).\n",
    "- Using **multiple filters** produces multiple output feature maps.\n",
    "\n",
    "### Stride\n",
    "**Stride (S)** is how many pixels the filter moves each step.\n",
    "- Stride 1: move one pixel at a time (dense scanning).\n",
    "- Stride 2: skip every other pixel (downsamples output).\n",
    "\n",
    "**Effect:** Higher stride → smaller output and faster computation, but may lose fine detail.\n",
    "\n",
    "### Padding\n",
    "**Padding (P)** adds extra border around the input (often zeros).\n",
    "- Without padding, output shrinks after convolution.\n",
    "- With padding, we can control output size and keep border information.\n",
    "\n",
    "Common padding styles:\n",
    "- **Valid padding:** P = 0 (no padding) → output smaller\n",
    "- **Same padding:** choose P so output size ≈ input size (often for stride 1)\n",
    "\n",
    "### Output size formula\n",
    "For input size \\(N\\), kernel size \\(K\\), padding \\(P\\), stride \\(S\\):\n",
    "\n",
    "\\[\n",
    "\\text{Output size} = \\left\\lfloor \\frac{N - K + 2P}{S} \\right\\rfloor + 1\n",
    "\\]\n",
    "\n",
    "For 2D images, apply the formula to height and width separately.\n",
    "\n",
    "**Example:** Input 32×32, K=3, S=1  \n",
    "- Valid padding (P=0): output = 30×30  \n",
    "- Same padding (P=1): output = 32×32  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bd004",
   "metadata": {},
   "source": [
    "## Q4) Describe the purpose of pooling layers in CNNs. Compare max pooling and average pooling operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b763288",
   "metadata": {},
   "source": [
    "\n",
    "### What is pooling?\n",
    "A **pooling layer** reduces the spatial resolution (height/width) of feature maps by summarizing local neighborhoods.\n",
    "\n",
    "### Purpose of pooling\n",
    "1. **Downsampling**\n",
    "   - Reduces computation and memory.\n",
    "2. **Makes features more robust**\n",
    "   - Small translations/noise have less effect (approx. translation invariance).\n",
    "3. **Helps reduce overfitting**\n",
    "   - Fewer activations, simpler representation.\n",
    "\n",
    "Pooling usually works on small windows like 2×2 with stride 2.\n",
    "\n",
    "### Max pooling vs Average pooling\n",
    "| Pooling type | Operation | Effect | When it helps |\n",
    "|---|---|---|---|\n",
    "| **Max pooling** | take maximum value in window | keeps strongest activation (feature presence) | common default for feature detection |\n",
    "| **Average pooling** | take average value in window | smooths feature map | useful when overall intensity matters; used in global average pooling |\n",
    "\n",
    "**Intuition:**\n",
    "- Max pooling asks: “Is this feature present anywhere in this region?”\n",
    "- Average pooling asks: “How strong is this feature on average in this region?”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd5646",
   "metadata": {},
   "source": [
    "---\n",
    "# Code Demos (NumPy)\n",
    "These short demos are included to make the theory concrete.\n",
    "---\n",
    "## Demo 1: Grayscale vs RGB representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84f6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b76b9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayscale shape: (2, 3)\n",
      "[[  0  50 100]\n",
      " [150 200 255]]\n",
      "\n",
      "RGB shape: (2, 3, 3)\n",
      "[[[255   0   0]\n",
      "  [  0 255   0]\n",
      "  [  0   0 255]]\n",
      "\n",
      " [[255 255   0]\n",
      "  [  0 255 255]\n",
      "  [255   0 255]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grayscale image: H x W\n",
    "gray = np.array([[0,  50, 100],\n",
    "                 [150, 200, 255]], dtype=np.uint8)\n",
    "\n",
    "# RGB image: H x W x 3 (R,G,B)\n",
    "rgb = np.array([[[255, 0,   0],   [0, 255, 0],   [0, 0, 255]],\n",
    "                [[255,255, 0],   [0, 255,255],  [255,0,255]]], dtype=np.uint8)\n",
    "\n",
    "print(\"Grayscale shape:\", gray.shape)\n",
    "print(gray)\n",
    "print(\"\\nRGB shape:\", rgb.shape)\n",
    "print(rgb)\n",
    "\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(gray, cmap=\"gray\")\n",
    "plt.title(\"Grayscale\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(rgb)\n",
    "plt.title(\"RGB\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc30c5c",
   "metadata": {},
   "source": [
    "## Demo 2: A simple 2D convolution (edge-detection style)\n",
    "We convolve a 5×5 image with a 3×3 kernel (valid padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29b74d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (5, 5)\n",
      "Kernel shape: (3, 3)\n",
      "Output (valid) shape: (3, 3)\n",
      "[[ 40.  80. 120.]\n",
      " [ 40.  80. 120.]\n",
      " [  0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def conv2d_valid(image, kernel):\n",
    "    \"\"\"Simple valid (no padding) 2D convolution for demo purposes.\"\"\"\n",
    "    H, W = image.shape\n",
    "    kH, kW = kernel.shape\n",
    "    outH = H - kH + 1\n",
    "    outW = W - kW + 1\n",
    "    out = np.zeros((outH, outW), dtype=float)\n",
    "    for i in range(outH):\n",
    "        for j in range(outW):\n",
    "            patch = image[i:i+kH, j:j+kW]\n",
    "            out[i, j] = np.sum(patch * kernel)\n",
    "    return out\n",
    "\n",
    "img = np.array([\n",
    "    [10, 10, 10, 10, 10],\n",
    "    [10, 10, 10, 10, 10],\n",
    "    [10, 10, 50, 50, 50],\n",
    "    [10, 10, 50, 50, 50],\n",
    "    [10, 10, 50, 50, 50],\n",
    "], dtype=float)\n",
    "\n",
    "kernel = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [ 0,  0,  0],\n",
    "    [ 1,  1,  1],\n",
    "], dtype=float)\n",
    "\n",
    "out = conv2d_valid(img, kernel)\n",
    "\n",
    "print(\"Input image shape:\", img.shape)\n",
    "print(\"Kernel shape:\", kernel.shape)\n",
    "print(\"Output (valid) shape:\", out.shape)\n",
    "print(out)\n",
    "\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(\"Input\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(kernel, cmap=\"gray\")\n",
    "plt.title(\"Kernel\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(out, cmap=\"gray\")\n",
    "plt.title(\"Convolved Output\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a49ed",
   "metadata": {},
   "source": [
    "## Demo 3: Output size with padding and stride\n",
    "We implement:\n",
    "\n",
    "\\[\\text{out} = \\left\\lfloor \\frac{N - K + 2P}{S} \\right\\rfloor + 1\\]\n",
    "\n",
    "and test a few cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05286d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid, stride 1 => out = 30\n",
      "same-ish, stride 1 => out = 32\n",
      "stride 2 (downsample) => out = 16\n",
      "valid 5x5 => out = 24\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def out_size(N, K, P=0, S=1):\n",
    "    return math.floor((N - K + 2*P)/S) + 1\n",
    "\n",
    "cases = [\n",
    "    {\"N\": 32, \"K\": 3, \"P\": 0, \"S\": 1, \"desc\": \"valid, stride 1\"},\n",
    "    {\"N\": 32, \"K\": 3, \"P\": 1, \"S\": 1, \"desc\": \"same-ish, stride 1\"},\n",
    "    {\"N\": 32, \"K\": 3, \"P\": 1, \"S\": 2, \"desc\": \"stride 2 (downsample)\"},\n",
    "    {\"N\": 28, \"K\": 5, \"P\": 0, \"S\": 1, \"desc\": \"valid 5x5\"},\n",
    "]\n",
    "\n",
    "for c in cases:\n",
    "    print(c[\"desc\"], \"=> out =\", out_size(c[\"N\"], c[\"K\"], c[\"P\"], c[\"S\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026932bb",
   "metadata": {},
   "source": [
    "## Demo 4: Max pooling vs Average pooling\n",
    "We pool a 4×4 feature map using 2×2 windows with stride 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c41b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature map:\n",
      " [[ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12.]\n",
      " [13. 14. 15. 16.]]\n",
      "\n",
      "Max pool 2x2 stride2:\n",
      " [[ 6.  8.]\n",
      " [14. 16.]]\n",
      "\n",
      "Avg pool 2x2 stride2:\n",
      " [[ 3.5  5.5]\n",
      " [11.5 13.5]]\n"
     ]
    }
   ],
   "source": [
    "def max_pool2x2(mat):\n",
    "    out = np.zeros((mat.shape[0]//2, mat.shape[1]//2))\n",
    "    for i in range(0, mat.shape[0], 2):\n",
    "        for j in range(0, mat.shape[1], 2):\n",
    "            out[i//2, j//2] = np.max(mat[i:i+2, j:j+2])\n",
    "    return out\n",
    "\n",
    "def avg_pool2x2(mat):\n",
    "    out = np.zeros((mat.shape[0]//2, mat.shape[1]//2))\n",
    "    for i in range(0, mat.shape[0], 2):\n",
    "        for j in range(0, mat.shape[1], 2):\n",
    "            out[i//2, j//2] = np.mean(mat[i:i+2, j:j+2])\n",
    "    return out\n",
    "\n",
    "feat = np.array([\n",
    "    [1,  2,  3,  4],\n",
    "    [5,  6,  7,  8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13,14, 15,16]\n",
    "], dtype=float)\n",
    "\n",
    "mx = max_pool2x2(feat)\n",
    "av = avg_pool2x2(feat)\n",
    "\n",
    "print(\"Feature map:\\n\", feat)\n",
    "print(\"\\nMax pool 2x2 stride2:\\n\", mx)\n",
    "print(\"\\nAvg pool 2x2 stride2:\\n\", av)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
