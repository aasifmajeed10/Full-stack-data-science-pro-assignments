{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e2bcdb",
   "metadata": {},
   "source": [
    "# ML Assignment – 2  \n",
    "**Submitter Name:** Aasif Majeed  \n",
    "**Date:** 24 May 2024  \n",
    "\n",
    "This notebook answers **all questions (1–80)** from the uploaded assignment PDF.  \n",
    "Each question is written with its number, followed by a clear explanation.  \n",
    "A few **optional Python demo cells** are included at the end to support key concepts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2dd70",
   "metadata": {},
   "source": [
    "---\n",
    "## 0) Imports (for optional demo code)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e7c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 140)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111fe5e",
   "metadata": {},
   "source": [
    "---\n",
    "# Answers (1–80)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed35c40",
   "metadata": {},
   "source": [
    "## Q1) What is regression analysis?\n",
    "\n",
    "Regression analysis is a **supervised learning** (and classical statistics) method that models the relationship between:\n",
    "- **Inputs / predictors**: \\(X = (x_1, x_2, \\dots, x_p)\\)\n",
    "- **Output / target**: \\(y\\) (continuous)\n",
    "\n",
    "**Core goals**\n",
    "1. **Prediction**: estimate \\(y\\) for new/unseen \\(X\\).\n",
    "2. **Explanation**: quantify how each predictor is associated with changes in \\(y\\) (e.g., “holding other variables constant”).\n",
    "\n",
    "**General form**\n",
    "\\[\n",
    "y = f(X) + \\epsilon\n",
    "\\]\n",
    "where \\(f\\) is the learned function and \\(\\epsilon\\) represents noise/unobserved factors.\n",
    "\n",
    "**Examples**\n",
    "- Predict house price from size, location features, and age.\n",
    "- Predict energy consumption from temperature and occupancy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a70818",
   "metadata": {},
   "source": [
    "## Q2) Explain the difference between linear and nonlinear regression.\n",
    "\n",
    "**Linear regression** assumes the model is linear in its parameters:\n",
    "\\[\n",
    "y \\approx \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n",
    "\\]\n",
    "Even if features are transformed (e.g., \\(\\log x\\)), it is still “linear regression” as long as it is linear in \\(\\beta\\).\n",
    "\n",
    "**Nonlinear regression** means the relationship is not linear (often nonlinear in parameters or structure), for example:\n",
    "- Exponential: \\(y = a e^{bx}\\)\n",
    "- Power law: \\(y = a x^b\\)\n",
    "- Saturation curves, logistic growth, etc.\n",
    "\n",
    "**Practical difference**\n",
    "- Linear models are easier to interpret and optimize (closed-form solutions exist).\n",
    "- Nonlinear models can fit complex patterns but may require iterative optimization and can overfit more easily without controls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc03e8a",
   "metadata": {},
   "source": [
    "## Q3) What is the difference between simple linear regression and multiple linear regression?\n",
    "\n",
    "**Simple linear regression** uses exactly **one predictor**:\n",
    "\\[\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "\\]\n",
    "It captures a straight-line relationship between one input and the target.\n",
    "\n",
    "**Multiple linear regression** uses **two or more predictors**:\n",
    "\\[\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p + \\epsilon\n",
    "\\]\n",
    "It models the combined effect of several variables (and can include interaction terms like \\(x_1x_2\\)).\n",
    "\n",
    "**Why multiple regression matters**\n",
    "- Real problems often have multiple drivers.\n",
    "- It lets you estimate the effect of one variable **while controlling for others**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecbb964",
   "metadata": {},
   "source": [
    "## Q4) How is the performance of a regression model typically evaluated?\n",
    "\n",
    "Regression performance is usually evaluated on **unseen data** (validation/test) using error metrics and diagnostics.\n",
    "\n",
    "**Common metrics**\n",
    "- **MAE**: \\(\\frac{1}{n}\\sum|y-\\hat{y}|\\) (robust, easy to interpret).\n",
    "- **MSE**: \\(\\frac{1}{n}\\sum(y-\\hat{y})^2\\) (penalizes large errors).\n",
    "- **RMSE**: \\(\\sqrt{\\text{MSE}}\\) (same units as \\(y\\)).\n",
    "- **R²**: \\(1 - \\frac{\\sum (y-\\hat{y})^2}{\\sum (y-\\bar{y})^2}\\) (variance explained).\n",
    "\n",
    "**Diagnostics**\n",
    "- Residual plots: check nonlinearity, heteroscedasticity, outliers.\n",
    "- Learning curves / CV: check generalization stability.\n",
    "\n",
    "**Best practice**\n",
    "- Do cross-validation for stable estimates, especially with smaller datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6a1a1",
   "metadata": {},
   "source": [
    "## Q5) What is overfitting in the context of regression models?\n",
    "\n",
    "**Overfitting** in regression happens when the model fits the **noise** in the training data instead of the true underlying pattern.\n",
    "\n",
    "**Symptoms**\n",
    "- Very low training error, but noticeably higher validation/test error.\n",
    "- Residuals look “too good” on train but unstable on new data.\n",
    "\n",
    "**Common causes**\n",
    "- Model too complex (high-degree polynomial, too many parameters).\n",
    "- Too many features relative to data size.\n",
    "- Data leakage (test info accidentally used in training).\n",
    "- Too long training without regularization (in iterative models).\n",
    "\n",
    "**Fixes**\n",
    "- Use regularization (Ridge/Lasso), reduce complexity, do feature selection, collect more data, and tune using cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b120872",
   "metadata": {},
   "source": [
    "## Q6) What is logistic regression used for?\n",
    "\n",
    "**Logistic regression** is primarily used for **classification**, especially:\n",
    "- **Binary classification**: \\(y \\in \\{0,1\\}\\)\n",
    "- Also extensions: multinomial logistic regression for multiple classes.\n",
    "\n",
    "It models the probability of the positive class:\n",
    "\\[\n",
    "p = P(y=1|x)\n",
    "\\]\n",
    "and then predicts class labels using a threshold (commonly 0.5).\n",
    "\n",
    "**Applications**\n",
    "- spam vs not spam\n",
    "- disease present vs absent\n",
    "- churn vs no churn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d3853",
   "metadata": {},
   "source": [
    "## Q7) How does logistic regression differ from linear regression?\n",
    "\n",
    "**Linear regression**\n",
    "- Target is continuous (\\(y \\in \\mathbb{R}\\)).\n",
    "- Output can be any real number.\n",
    "- Often optimized with least squares (MSE).\n",
    "\n",
    "**Logistic regression**\n",
    "- Target is categorical (typically 0/1).\n",
    "- Output is a probability \\(p \\in (0,1)\\) via the sigmoid.\n",
    "- Optimized with **log-loss / cross-entropy** (maximum likelihood).\n",
    "\n",
    "**Important note**\n",
    "Despite the name, logistic regression is a **classification** model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d770e",
   "metadata": {},
   "source": [
    "## Q8) Explain the concept of odds ratio in logistic regression.\n",
    "\n",
    "In logistic regression, we often work with **odds** and **log-odds**:\n",
    "\n",
    "- Probability: \\(p = P(y=1|x)\\)\n",
    "- **Odds**: \\(\\frac{p}{1-p}\\)\n",
    "- **Log-odds (logit)**:\n",
    "\\[\n",
    "\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta^T x\n",
    "\\]\n",
    "\n",
    "**Odds ratio (OR)**\n",
    "For a 1-unit increase in feature \\(x_j\\), the odds multiply by:\n",
    "\\[\n",
    "OR = e^{\\beta_j}\n",
    "\\]\n",
    "Interpretation:\n",
    "- \\(OR>1\\): odds increase\n",
    "- \\(OR<1\\): odds decrease\n",
    "- \\(OR=1\\): no change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65d4b1",
   "metadata": {},
   "source": [
    "## Q9) What is the sigmoid function in logistic regression?\n",
    "\n",
    "The **sigmoid** (logistic) function converts any real number into a probability in (0,1):\n",
    "\\[\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "\\]\n",
    "where \\(z = \\beta_0 + \\beta^T x\\).\n",
    "\n",
    "**Why it matters**\n",
    "- It “squashes” outputs to be valid probabilities.\n",
    "- It creates an S-shaped curve: small changes near 0 can change probability a lot; far from 0, the probability saturates near 0 or 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b20e46",
   "metadata": {},
   "source": [
    "## Q10) How is the performance of a logistic regression model evaluated?\n",
    "\n",
    "Logistic regression performance is evaluated using **classification metrics**, ideally on validation/test data.\n",
    "\n",
    "**Core tools**\n",
    "- **Confusion matrix**: TP, FP, TN, FN\n",
    "- **Accuracy**: may be misleading if data is imbalanced\n",
    "- **Precision**: TP/(TP+FP)\n",
    "- **Recall**: TP/(TP+FN)\n",
    "- **F1-score**: harmonic mean of precision and recall\n",
    "- **ROC-AUC**: ranking quality across thresholds\n",
    "- **PR-AUC**: often better for imbalanced datasets\n",
    "- **Log-loss (cross-entropy)**: evaluates probability quality\n",
    "\n",
    "**Extra**\n",
    "- Threshold tuning and calibration checks are common when probabilities matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840e6b0",
   "metadata": {},
   "source": [
    "## Q11) What is a decision tree?\n",
    "\n",
    "A **decision tree** is a model that makes predictions by repeatedly applying **if–else** rules on features.\n",
    "\n",
    "- In **classification**, each leaf predicts a class (often majority class) or class probabilities.\n",
    "- In **regression**, each leaf predicts a numeric value (often mean of y in that leaf).\n",
    "\n",
    "Trees are popular because they:\n",
    "- handle nonlinear relationships,\n",
    "- work with mixed feature types,\n",
    "- are interpretable in small-to-medium depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6006f",
   "metadata": {},
   "source": [
    "## Q12) How does a decision tree make predictions?\n",
    "\n",
    "A decision tree predicts by **traversing** from the root to a leaf:\n",
    "\n",
    "1. Start at root node.\n",
    "2. Apply the split rule (example: `feature_j <= threshold`).\n",
    "3. Depending on the rule outcome, go left or right.\n",
    "4. Repeat until reaching a leaf node.\n",
    "5. Output the leaf’s prediction (class label/probability or mean value).\n",
    "\n",
    "The training algorithm chooses splits that best reduce impurity (classification) or reduce variance/error (regression).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a7cef",
   "metadata": {},
   "source": [
    "## Q13) What is entropy in the context of decision trees?\n",
    "\n",
    "**Entropy** measures class impurity/uncertainty at a node in a classification tree:\n",
    "\n",
    "\\[\n",
    "H(S) = -\\sum_{k=1}^{K} p_k \\log_2(p_k)\n",
    "\\]\n",
    "where \\(p_k\\) is the fraction of samples in class \\(k\\).\n",
    "\n",
    "- \\(H=0\\) when the node is **pure** (all samples in one class).\n",
    "- Entropy is higher when classes are mixed.\n",
    "\n",
    "Trees select splits using **information gain** (entropy reduction):\n",
    "\\[\n",
    "IG = H(parent) - \\sum_{child} \\frac{n_{child}}{n_{parent}} H(child)\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbf47b",
   "metadata": {},
   "source": [
    "## Q14) What is pruning in decision trees?\n",
    "\n",
    "**Pruning** reduces a decision tree’s complexity to improve generalization and prevent overfitting.\n",
    "\n",
    "**Pre-pruning (early stopping)**\n",
    "- stop splitting early using constraints like:\n",
    "  - `max_depth`\n",
    "  - `min_samples_split`\n",
    "  - `min_samples_leaf`\n",
    "\n",
    "**Post-pruning**\n",
    "- build a large tree, then remove weak branches using criteria like cost-complexity pruning.\n",
    "\n",
    "**Effect**\n",
    "- simpler tree, lower variance, often better test performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802667ae",
   "metadata": {},
   "source": [
    "## Q15) How do decision trees handle missing values?\n",
    "\n",
    "Decision trees can handle missing values in several ways (depends on implementation/library):\n",
    "\n",
    "1. **Imputation before training** (most common): fill missing with mean/median/mode.\n",
    "2. **Treat missing as its own category** (categorical variables).\n",
    "3. **Surrogate splits**: if best split feature is missing, use another correlated feature’s split.\n",
    "4. Some boosted-tree libraries learn a default direction for missing values at each split.\n",
    "\n",
    "**Best practice**\n",
    "- Use a pipeline: fit imputer on training data only, then transform validation/test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ade96",
   "metadata": {},
   "source": [
    "## Q16) What is a support vector machine (SVM)?\n",
    "\n",
    "A **Support Vector Machine (SVM)** is a supervised model that finds a decision boundary with maximum margin.\n",
    "\n",
    "For binary classification, it tries to find a hyperplane:\n",
    "\\[\n",
    "w^T x + b = 0\n",
    "\\]\n",
    "that separates classes while maximizing the margin (distance from the hyperplane to nearest points).\n",
    "\n",
    "Variants:\n",
    "- **SVC**: classification\n",
    "- **SVR**: regression\n",
    "- Supports **kernels** for nonlinear decision boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc02a9d",
   "metadata": {},
   "source": [
    "## Q17) Explain the concept of margin in SVM.\n",
    "\n",
    "The **margin** is the distance between the separating hyperplane and the nearest points from each class.\n",
    "\n",
    "For a linear SVM:\n",
    "- The hyperplane is \\(w^T x + b = 0\\).\n",
    "- The margin is proportional to \\(1/\\|w\\|\\).\n",
    "SVM maximizes this margin because larger margin often leads to better generalization (less sensitive to small perturbations).\n",
    "\n",
    "In soft-margin SVM, we allow some violations using slack variables (controlled by \\(C\\)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57bea2a",
   "metadata": {},
   "source": [
    "## Q18) What are support vectors in SVM?\n",
    "\n",
    "**Support vectors** are the training data points that lie closest to the decision boundary (or on/inside the margin).\n",
    "\n",
    "Why they matter:\n",
    "- They **completely determine** the SVM boundary.\n",
    "- If you remove non-support-vector points, the boundary usually stays the same.\n",
    "- They represent the “hardest” points to classify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db024e",
   "metadata": {},
   "source": [
    "## Q19) How does SVM handle non-linearly separable data?\n",
    "\n",
    "SVM handles nonlinearly separable data in two main ways:\n",
    "\n",
    "1. **Soft margin** (slack variables): allow some points to be misclassified or inside the margin.\n",
    "   - Controlled by parameter \\(C\\) (higher \\(C\\) → less tolerance for errors, risk more overfitting).\n",
    "2. **Kernel trick**: implicitly maps inputs to a higher-dimensional space where a linear separator becomes possible.\n",
    "   - Common kernels: **RBF (Gaussian)**, polynomial, sigmoid.\n",
    "\n",
    "This allows SVM to model nonlinear boundaries while keeping computation manageable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554fc66",
   "metadata": {},
   "source": [
    "## Q20) What are the advantages of SVM over other classification algorithms?\n",
    "\n",
    "**Advantages of SVM**\n",
    "- Strong performance in **high-dimensional** spaces.\n",
    "- Effective with clear margins between classes.\n",
    "- Kernel trick enables flexible nonlinear classification.\n",
    "- Uses only support vectors to define boundary (compact boundary representation).\n",
    "\n",
    "**Limitations**\n",
    "- Can be slower for very large datasets.\n",
    "- Sensitive to feature scaling (standardization recommended).\n",
    "- Requires tuning (\\(C\\), kernel parameters like \\(\\gamma\\)).\n",
    "- Not inherently probabilistic; probabilities need calibration (`probability=True` or Platt scaling).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27234a43",
   "metadata": {},
   "source": [
    "## Q21) What is the Naïve Bayes algorithm?\n",
    "\n",
    "**Naïve Bayes (NB)** is a probabilistic classifier based on Bayes’ theorem:\n",
    "\\[\n",
    "P(y|x) \\propto P(y)\\,P(x|y)\n",
    "\\]\n",
    "and the **naïve assumption**:\n",
    "\\[\n",
    "P(x|y) = \\prod_{j=1}^{p} P(x_j|y)\n",
    "\\]\n",
    "\n",
    "So the prediction rule is:\n",
    "\\[\n",
    "\\hat{y} = \\arg\\max_y \\left[\\log P(y) + \\sum_j \\log P(x_j|y)\\right]\n",
    "\\]\n",
    "\n",
    "NB is fast, simple, and works especially well for text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19715450",
   "metadata": {},
   "source": [
    "## Q22) Why is it called 'Naïve' Bayes?\n",
    "\n",
    "It is called **“naïve”** because it assumes that all features are **conditionally independent** given the class label \\(y\\).\n",
    "\n",
    "Example: In spam detection, NB treats words as independent given spam/not spam.  \n",
    "This is not strictly true in language, but the assumption often still produces good performance because:\n",
    "- many features contribute additive evidence,\n",
    "- errors may cancel out,\n",
    "- it generalizes well in sparse high-dimensional spaces (like bag-of-words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9203e8f",
   "metadata": {},
   "source": [
    "## Q23) How does Naïve Bayes handle continuous and categorical features?\n",
    "\n",
    "Naïve Bayes uses different likelihood models depending on feature type:\n",
    "\n",
    "**1) Continuous features**\n",
    "- **Gaussian NB** assumes each feature follows a normal distribution within each class:\n",
    "  \\[\n",
    "  x_j|y \\sim \\mathcal{N}(\\mu_{jy}, \\sigma_{jy}^2)\n",
    "  \\]\n",
    "\n",
    "**2) Categorical / count features**\n",
    "- **Multinomial NB**: word counts / frequencies (text).\n",
    "- **Bernoulli NB**: binary features (word present/absent).\n",
    "- **Categorical NB**: general categorical features with probability tables.\n",
    "\n",
    "Smoothing (e.g., Laplace) is often used for categorical/count features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3c166",
   "metadata": {},
   "source": [
    "## Q24) Explain the concept of prior and posterior probabilities in Naïve Bayes.\n",
    "\n",
    "**Prior probability**: \\(P(y)\\)  \n",
    "- The probability of class \\(y\\) before seeing any features.\n",
    "- Example: if 70% emails are not spam, \\(P(y=\\text{not spam})=0.7\\).\n",
    "\n",
    "**Likelihood**: \\(P(x|y)\\)  \n",
    "- Probability of observing features \\(x\\) given class \\(y\\).\n",
    "\n",
    "**Posterior probability**: \\(P(y|x)\\)  \n",
    "- Updated probability after seeing features:\n",
    "\\[\n",
    "P(y|x) = \\frac{P(x|y)\\,P(y)}{P(x)}\n",
    "\\]\n",
    "In classification, we compare posteriors across classes and pick the largest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbdbd7",
   "metadata": {},
   "source": [
    "## Q25) What is Laplace smoothing and why is it used in Naïve Bayes?\n",
    "\n",
    "**Laplace smoothing** (add-one smoothing) prevents zero probabilities when a category/word is unseen in training for a class.\n",
    "\n",
    "Without smoothing:\n",
    "- If \\(P(x_j|y)=0\\) for any feature, the entire product becomes 0 → model fails.\n",
    "\n",
    "With Laplace smoothing:\n",
    "\\[\n",
    "P(w|c) = \\frac{count(w,c)+\\alpha}{\\sum_w count(w,c) + \\alpha V}\n",
    "\\]\n",
    "- \\(V\\) = vocabulary size (or number of categories)\n",
    "- \\(\\alpha=1\\) gives classic Laplace smoothing\n",
    "\n",
    "This improves robustness, especially in sparse text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79211785",
   "metadata": {},
   "source": [
    "## Q26) Can Naïve Bayes be used for regression tasks?\n",
    "\n",
    "In standard ML practice, Naïve Bayes is mainly a **classification** algorithm.\n",
    "\n",
    "- Some research variants exist for regression, but they are not commonly used in mainstream pipelines.\n",
    "- For regression tasks, common choices are:\n",
    "  - Linear regression / Ridge / Lasso\n",
    "  - SVR\n",
    "  - Random Forest Regressor\n",
    "  - Gradient Boosting Regressor (XGBoost/LightGBM)\n",
    "\n",
    "So the typical answer: **NB is not a standard regression model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114a0a7",
   "metadata": {},
   "source": [
    "## Q27) How do you handle missing values in Naïve Bayes?\n",
    "\n",
    "Handling missing values for Naïve Bayes is usually done via **preprocessing**, because most NB implementations expect complete input.\n",
    "\n",
    "Common strategies:\n",
    "- **Impute numeric** features with mean/median.\n",
    "- **Impute categorical** with mode or treat missing as its own category like `\"Unknown\"`.\n",
    "- Add **missingness indicator features** (0/1 flags) to capture informative missingness.\n",
    "\n",
    "Important: fit imputation on **training data only** to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb7de2",
   "metadata": {},
   "source": [
    "## Q28) What are some common applications of Naïve Bayes?\n",
    "\n",
    "Naïve Bayes is widely used in problems where features are many and sparse, especially text.\n",
    "\n",
    "Common applications:\n",
    "- **Spam detection**\n",
    "- **Sentiment analysis**\n",
    "- **Document/topic classification**\n",
    "- **Language detection**\n",
    "- **Medical diagnosis baselines** (quick baseline)\n",
    "- **Real-time classification** where speed matters\n",
    "\n",
    "It is often used as a strong baseline because it trains and predicts very fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759045c8",
   "metadata": {},
   "source": [
    "## Q29) Explain the concept of feature independence assumption in Naïve Bayes.\n",
    "\n",
    "The key assumption in Naïve Bayes is **conditional independence**:\n",
    "\\[\n",
    "P(x_1, x_2, \\dots, x_p \\mid y) = \\prod_{j=1}^{p} P(x_j \\mid y)\n",
    "\\]\n",
    "Meaning: once we know the class \\(y\\), knowing \\(x_1\\) gives no extra information about \\(x_2\\), etc.\n",
    "\n",
    "Why it helps:\n",
    "- reduces the number of parameters to estimate,\n",
    "- makes learning and inference fast and stable in high dimensions.\n",
    "\n",
    "Why it can fail:\n",
    "- when strong dependencies between features carry important information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8619564",
   "metadata": {},
   "source": [
    "## Q30) How does Naïve Bayes handle categorical features with a large number of categories?\n",
    "\n",
    "Large-category features (high cardinality) can be challenging because:\n",
    "- probability tables become large,\n",
    "- many categories are rare → unreliable estimates.\n",
    "\n",
    "Ways NB handles it:\n",
    "- **Smoothing** (Laplace/add-α) to avoid zero probabilities and reduce overconfidence.\n",
    "- **Grouping rare categories** into an `\"Other\"` bucket.\n",
    "- **Feature hashing** to reduce dimensionality (common in text pipelines).\n",
    "- Prefer count-based representations (Multinomial NB) for text-like features.\n",
    "\n",
    "Goal: keep estimates stable and avoid huge sparse matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed51c09",
   "metadata": {},
   "source": [
    "## Q31) What is the curse of dimensionality, and how does it affect machine learning algorithms?\n",
    "\n",
    "The **curse of dimensionality** describes how data becomes sparse and harder to model as the number of features grows.\n",
    "\n",
    "What happens:\n",
    "- volume of space grows exponentially → need much more data to cover it\n",
    "- distances become less informative (kNN, k-means suffer)\n",
    "- models overfit more easily (many degrees of freedom)\n",
    "\n",
    "How to mitigate:\n",
    "- feature selection (remove irrelevant/redundant features)\n",
    "- dimensionality reduction (PCA/UMAP)\n",
    "- regularization (Ridge/Lasso)\n",
    "- get more data or better features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb8785",
   "metadata": {},
   "source": [
    "## Q32) Explain the bias-variance tradeoff and its implications for machine learning models.\n",
    "\n",
    "**Bias–variance tradeoff** explains why making a model more complex is not always better.\n",
    "\n",
    "- **Bias**: error from overly simple assumptions (underfitting).  \n",
    "  High bias → poor training and test performance.\n",
    "- **Variance**: error from sensitivity to training data (overfitting).  \n",
    "  High variance → excellent training but poor test performance.\n",
    "\n",
    "As complexity increases:\n",
    "- bias ↓\n",
    "- variance ↑\n",
    "\n",
    "We choose complexity/regularization to minimize **validation/test error**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c014d0",
   "metadata": {},
   "source": [
    "## Q33) What is cross-validation, and why is it used?\n",
    "\n",
    "**Cross-validation (CV)** evaluates a model by splitting data into multiple train/validation folds.\n",
    "\n",
    "In **k-fold CV**:\n",
    "1. Split dataset into k equal folds.\n",
    "2. Train on k−1 folds, validate on the remaining fold.\n",
    "3. Repeat k times and average performance.\n",
    "\n",
    "Why used:\n",
    "- more reliable estimate than a single split\n",
    "- better use of limited data\n",
    "- standard method for hyperparameter tuning and model selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bcdc5",
   "metadata": {},
   "source": [
    "## Q34) Explain the difference between parametric and non-parametric machine learning algorithms.\n",
    "\n",
    "**Parametric algorithms** assume a fixed functional form and learn a finite set of parameters.\n",
    "- Examples: linear regression, logistic regression, (Gaussian) Naïve Bayes.\n",
    "\n",
    "**Non-parametric algorithms** do not assume a fixed form; model complexity can grow with data.\n",
    "- Examples: kNN, decision trees, random forests, kernel density methods.\n",
    "\n",
    "Tradeoff:\n",
    "- parametric: simpler, faster, may underfit\n",
    "- non-parametric: flexible, can overfit, often needs more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65fe8cd",
   "metadata": {},
   "source": [
    "## Q35) What is feature scaling, and why is it important in machine learning?\n",
    "\n",
    "**Feature scaling** transforms features to comparable ranges (e.g., standardization or min-max scaling).\n",
    "\n",
    "Why important:\n",
    "- **Distance-based models** (kNN, k-means) depend heavily on scales.\n",
    "- **Gradient-based models** (logistic regression, neural nets) converge faster with scaled inputs.\n",
    "- **SVM** is sensitive to scaling, especially with RBF kernel.\n",
    "- **PCA** requires scaling to avoid domination by large-scale features.\n",
    "\n",
    "Tree-based models are less sensitive, but scaling still helps in mixed pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830de91",
   "metadata": {},
   "source": [
    "## Q36) What is regularization, and why is it used in machine learning?\n",
    "\n",
    "**Regularization** adds a penalty term to the loss function to discourage overly complex models and prevent overfitting.\n",
    "\n",
    "Common types:\n",
    "- **L2 (Ridge)**: \\(\\lambda \\sum \\beta_j^2\\)  \n",
    "  shrinks coefficients smoothly; good for multicollinearity.\n",
    "- **L1 (Lasso)**: \\(\\lambda \\sum |\\beta_j|\\)  \n",
    "  encourages sparsity → performs feature selection.\n",
    "\n",
    "Benefits:\n",
    "- improves generalization\n",
    "- stabilizes coefficients\n",
    "- reduces variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9d551",
   "metadata": {},
   "source": [
    "## Q37) Explain the concept of ensemble learning and give an example.\n",
    "\n",
    "**Ensemble learning** combines multiple models to produce a stronger final predictor.\n",
    "\n",
    "Why it works:\n",
    "- multiple learners reduce variance and/or bias\n",
    "- errors can cancel out\n",
    "\n",
    "Examples:\n",
    "- **Random Forest** (bagging): many trees trained on bootstrapped samples.\n",
    "- **Gradient Boosting** (boosting): sequentially adds models to correct errors.\n",
    "- **Voting classifier**: majority vote/averaging across different models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afe82b",
   "metadata": {},
   "source": [
    "## Q38) What is the difference between bagging and boosting?\n",
    "\n",
    "**Bagging (Bootstrap Aggregating)**\n",
    "- Train many models in parallel on different bootstrapped datasets.\n",
    "- Combine predictions by averaging/voting.\n",
    "- Main effect: reduces **variance**.\n",
    "- Example: Random Forest.\n",
    "\n",
    "**Boosting**\n",
    "- Train models sequentially; each new model focuses on previous errors.\n",
    "- Combine via weighted sum.\n",
    "- Main effect: reduces **bias** (often improves accuracy).\n",
    "- Examples: AdaBoost, Gradient Boosting, XGBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e114f4e",
   "metadata": {},
   "source": [
    "## Q39) What is the difference between a generative model and a discriminative model?\n",
    "\n",
    "**Generative models** learn how data is generated by modeling \\(P(x,y)\\) or \\(P(x|y)\\) and \\(P(y)\\).\n",
    "- Example: Naïve Bayes (models \\(P(x|y)\\) and \\(P(y)\\)).\n",
    "- Can generate samples of \\(x\\) given \\(y\\) (in principle).\n",
    "\n",
    "**Discriminative models** directly model \\(P(y|x)\\) or learn the decision boundary.\n",
    "- Examples: logistic regression, SVM, neural networks.\n",
    "- Often achieve higher predictive accuracy when enough labeled data exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8c797",
   "metadata": {},
   "source": [
    "## Q40) Explain the concept of batch gradient descent and stochastic gradient descent.\n",
    "\n",
    "**Batch Gradient Descent**\n",
    "- Computes gradient using **all training samples** each step.\n",
    "- Stable updates, but expensive for large datasets.\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)**\n",
    "- Computes gradient using **one sample** at a time.\n",
    "- Very fast per step, but noisy updates (can bounce around).\n",
    "\n",
    "**Mini-batch GD**\n",
    "- Uses small batches (e.g., 32–256).\n",
    "- Most common in practice: balances stability and speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d502d92",
   "metadata": {},
   "source": [
    "## Q41) What is the K-nearest neighbors (KNN) algorithm, and how does it work?\n",
    "\n",
    "**k-Nearest Neighbors (kNN)** is a lazy, non-parametric method.\n",
    "\n",
    "How it works:\n",
    "1. Choose \\(k\\).\n",
    "2. For a query point, compute distance to training points (Euclidean, cosine, etc.).\n",
    "3. Pick the \\(k\\) nearest points.\n",
    "4. Predict:\n",
    "   - **classification:** majority vote (optionally distance-weighted)\n",
    "   - **regression:** mean/weighted mean of neighbors’ targets\n",
    "\n",
    "It relies on the idea that “similar points have similar labels.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00193e",
   "metadata": {},
   "source": [
    "## Q42) What are the disadvantages of the K-nearest neighbors algorithm?\n",
    "\n",
    "Main disadvantages of kNN:\n",
    "- **Slow prediction**: needs distances to many points (unless using indexing structures).\n",
    "- **Memory-heavy**: stores the full training set.\n",
    "- **Sensitive to scaling**: unscaled features distort distances.\n",
    "- **Curse of dimensionality**: distances become less meaningful in high dimensions.\n",
    "- Performance depends heavily on \\(k\\) choice and distance metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e62ca6",
   "metadata": {},
   "source": [
    "## Q43) Explain the concept of one-hot encoding and its use in machine learning.\n",
    "\n",
    "**One-hot encoding** converts a categorical feature into multiple binary columns.\n",
    "\n",
    "Example: Color ∈ {Red, Blue, Green}\n",
    "- Red → [1,0,0]\n",
    "- Blue → [0,1,0]\n",
    "- Green → [0,0,1]\n",
    "\n",
    "Why used:\n",
    "- avoids imposing a fake ordinal relationship (unlike label encoding).\n",
    "- enables linear models and many ML algorithms to process categorical inputs.\n",
    "\n",
    "Caution:\n",
    "- for high-cardinality categories, one-hot can create too many columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e701d",
   "metadata": {},
   "source": [
    "## Q44) What is feature selection, and why is it important in machine learning?\n",
    "\n",
    "**Feature selection** is choosing the most relevant subset of input features for a model.\n",
    "\n",
    "Why important:\n",
    "- reduces overfitting (removes noise)\n",
    "- improves generalization\n",
    "- reduces training time and storage\n",
    "- improves interpretability\n",
    "- reduces multicollinearity for linear models\n",
    "\n",
    "Approaches:\n",
    "- filter methods (correlation, chi-square)\n",
    "- wrapper methods (RFE, forward/backward selection)\n",
    "- embedded methods (Lasso, tree-based importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c4ad9",
   "metadata": {},
   "source": [
    "## Q45) Explain the concept of cross-entropy loss and its use in classification tasks.\n",
    "\n",
    "**Cross-entropy loss** measures how well predicted probabilities match true labels.\n",
    "For binary classification:\n",
    "\\[\n",
    "L = -\\left[y\\log(p) + (1-y)\\log(1-p)\\right]\n",
    "\\]\n",
    "- If the model is confidently wrong (p close to 0 when y=1), loss becomes very large.\n",
    "- Encourages well-calibrated probabilities.\n",
    "\n",
    "Used in:\n",
    "- logistic regression\n",
    "- neural networks for classification\n",
    "- many probabilistic classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e7a74",
   "metadata": {},
   "source": [
    "## Q46) What is the difference between batch learning and online learning?\n",
    "\n",
    "**Batch learning**\n",
    "- Train once on a fixed dataset.\n",
    "- To update with new data, you often retrain from scratch (or periodically).\n",
    "\n",
    "**Online learning**\n",
    "- Update model incrementally as new data arrives (streaming).\n",
    "- Useful for large datasets, real-time systems, or when data distribution changes (concept drift).\n",
    "\n",
    "Examples:\n",
    "- online SGD, incremental Naïve Bayes, streaming recommender systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98965e47",
   "metadata": {},
   "source": [
    "## Q47) Explain the concept of grid search and its use in hyperparameter tuning.\n",
    "\n",
    "**Grid search** tries all combinations of a specified set of hyperparameters.\n",
    "\n",
    "Example: for SVM (RBF), tune \\(C\\) and \\(\\gamma\\).\n",
    "- Use validation set or **cross-validation** to evaluate each combination.\n",
    "- Choose the one with best CV score.\n",
    "\n",
    "Pros:\n",
    "- simple and exhaustive\n",
    "\n",
    "Cons:\n",
    "- can be expensive when many parameters or wide ranges\n",
    "Alternative: random search, Bayesian optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe296543",
   "metadata": {},
   "source": [
    "## Q48) What are the advantages and disadvantages of decision trees?\n",
    "\n",
    "**Decision tree advantages**\n",
    "- interpretable (especially shallow trees)\n",
    "- handles nonlinearities and interactions\n",
    "- minimal preprocessing (no scaling needed)\n",
    "- works with numeric + categorical (with proper handling)\n",
    "\n",
    "**Disadvantages**\n",
    "- prone to overfitting (high variance)\n",
    "- unstable: small data changes can change structure\n",
    "- greedy split decisions may miss global optimum\n",
    "- can be biased toward features with many possible splits\n",
    "\n",
    "Fixes: pruning, ensembles (Random Forest, Gradient Boosting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d97c0",
   "metadata": {},
   "source": [
    "## Q49) What is the difference between L1 and L2 regularization?\n",
    "\n",
    "**L1 regularization (Lasso)**\n",
    "- Penalty: \\(\\lambda \\sum_j |\\beta_j|\\)\n",
    "- Encourages sparsity → some coefficients become exactly 0 (feature selection).\n",
    "\n",
    "**L2 regularization (Ridge)**\n",
    "- Penalty: \\(\\lambda \\sum_j \\beta_j^2\\)\n",
    "- Shrinks coefficients smoothly, rarely to exactly 0.\n",
    "- Works well when features are correlated (multicollinearity).\n",
    "\n",
    "Elastic Net combines both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988d4b6",
   "metadata": {},
   "source": [
    "## Q50) What are some common preprocessing techniques used in machine learning?\n",
    "\n",
    "Common preprocessing techniques include:\n",
    "- handling missing values (imputation + indicators)\n",
    "- encoding categorical variables (one-hot, ordinal, target encoding)\n",
    "- scaling/normalization (standardization, min-max)\n",
    "- outlier handling (IQR, z-score, robust scaling)\n",
    "- train/validation/test split (avoid leakage)\n",
    "- feature selection or dimensionality reduction (PCA)\n",
    "- class imbalance handling (resampling, SMOTE, class weights)\n",
    "- text preprocessing (tokenization, TF-IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4c197",
   "metadata": {},
   "source": [
    "## Q51) What is the difference between a parametric and non-parametric algorithm? Give examples of each.\n",
    "\n",
    "A **parametric** algorithm has a fixed number of parameters regardless of dataset size.\n",
    "- Examples: **linear regression**, **logistic regression**, **Naïve Bayes** (with distribution assumptions).\n",
    "\n",
    "A **non-parametric** algorithm adapts complexity with data size or structure.\n",
    "- Examples: **kNN**, **decision trees**, **random forests**.\n",
    "\n",
    "Key difference: parametric models are simpler and need fewer samples, while non-parametric models are flexible but may require more data and careful regularization/tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f7514",
   "metadata": {},
   "source": [
    "## Q52) Explain the bias-variance tradeoff and how it relates to model complexity.\n",
    "\n",
    "Bias–variance tradeoff relates directly to **model complexity**:\n",
    "- As complexity increases, **training error decreases**.\n",
    "- Bias tends to decrease, but variance tends to increase.\n",
    "\n",
    "Practical implication:\n",
    "- Very simple model → underfits (high bias)\n",
    "- Very complex model → overfits (high variance)\n",
    "\n",
    "We use validation curves, learning curves, and cross-validation to find the best complexity/regularization level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ee1a6",
   "metadata": {},
   "source": [
    "## Q53) What are the advantages and disadvantages of using ensemble methods like random forests?\n",
    "\n",
    "**Random Forest (an ensemble of decision trees)**\n",
    "\n",
    "Advantages:\n",
    "- strong performance on many tabular tasks\n",
    "- reduces overfitting compared to a single tree (bagging reduces variance)\n",
    "- handles nonlinearities and interactions well\n",
    "- provides feature importance estimates\n",
    "- robust to outliers and monotonic transforms\n",
    "\n",
    "Disadvantages:\n",
    "- less interpretable than a single tree\n",
    "- can be heavy (many trees) for memory and deployment\n",
    "- may underperform boosted methods on some tasks without tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f162eb",
   "metadata": {},
   "source": [
    "## Q54) Explain the difference between bagging and boosting.\n",
    "\n",
    "**Bagging (Bootstrap Aggregating)**\n",
    "- Train many models in parallel on different bootstrapped datasets.\n",
    "- Combine predictions by averaging/voting.\n",
    "- Main effect: reduces **variance**.\n",
    "- Example: Random Forest.\n",
    "\n",
    "**Boosting**\n",
    "- Train models sequentially; each new model focuses on previous errors.\n",
    "- Combine via weighted sum.\n",
    "- Main effect: reduces **bias** (often improves accuracy).\n",
    "- Examples: AdaBoost, Gradient Boosting, XGBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d0dfa",
   "metadata": {},
   "source": [
    "## Q55) What is the purpose of hyperparameter tuning in machine learning?\n",
    "\n",
    "Hyperparameter tuning finds the best “settings” that are not learned directly from the data (unlike model weights).\n",
    "Examples:\n",
    "- SVM: \\(C\\), kernel choice, \\(\\gamma\\)\n",
    "- Tree: max_depth, min_samples_leaf\n",
    "- kNN: \\(k\\)\n",
    "- Boosting: learning rate, number of estimators\n",
    "\n",
    "Purpose:\n",
    "- improve generalization\n",
    "- avoid underfitting/overfitting\n",
    "- produce stable, reproducible performance using validation/CV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5855e1d",
   "metadata": {},
   "source": [
    "## Q56) What is the difference between regularization and feature selection?\n",
    "\n",
    "**Regularization** controls model complexity by adding a penalty to the loss (keeps all features but shrinks coefficients).\n",
    "- Example: Ridge and Lasso penalties.\n",
    "\n",
    "**Feature selection** reduces complexity by **removing features** entirely.\n",
    "- Example: RFE, selecting top features by mutual information.\n",
    "\n",
    "They can be used together:\n",
    "- engineer features → select useful ones → regularize model for stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad25a98",
   "metadata": {},
   "source": [
    "## Q57) How does the Lasso (L1) regularization differ from Ridge (L2) regularization?\n",
    "\n",
    "**Lasso (L1)**:\n",
    "- adds \\(\\lambda \\sum |\\beta_j|\\)\n",
    "- can force some coefficients to exactly zero → automatic feature selection\n",
    "- useful when you believe only a few features matter\n",
    "\n",
    "**Ridge (L2)**:\n",
    "- adds \\(\\lambda \\sum \\beta_j^2\\)\n",
    "- shrinks coefficients but keeps them non-zero\n",
    "- very good when features are correlated and you want stable estimates\n",
    "\n",
    "**Choice** depends on sparsity expectations and multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39bd63",
   "metadata": {},
   "source": [
    "## Q58) Explain the concept of cross-validation and why it is used.\n",
    "\n",
    "**Cross-validation (CV)** evaluates a model by splitting data into multiple train/validation folds.\n",
    "\n",
    "In **k-fold CV**:\n",
    "1. Split dataset into k equal folds.\n",
    "2. Train on k−1 folds, validate on the remaining fold.\n",
    "3. Repeat k times and average performance.\n",
    "\n",
    "Why used:\n",
    "- more reliable estimate than a single split\n",
    "- better use of limited data\n",
    "- standard method for hyperparameter tuning and model selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0a9a1",
   "metadata": {},
   "source": [
    "## Q59) What are some common evaluation metrics used for regression tasks?\n",
    "\n",
    "Regression performance is usually evaluated on **unseen data** (validation/test) using error metrics and diagnostics.\n",
    "\n",
    "**Common metrics**\n",
    "- **MAE**: \\(\\frac{1}{n}\\sum|y-\\hat{y}|\\) (robust, easy to interpret).\n",
    "- **MSE**: \\(\\frac{1}{n}\\sum(y-\\hat{y})^2\\) (penalizes large errors).\n",
    "- **RMSE**: \\(\\sqrt{\\text{MSE}}\\) (same units as \\(y\\)).\n",
    "- **R²**: \\(1 - \\frac{\\sum (y-\\hat{y})^2}{\\sum (y-\\bar{y})^2}\\) (variance explained).\n",
    "\n",
    "**Diagnostics**\n",
    "- Residual plots: check nonlinearity, heteroscedasticity, outliers.\n",
    "- Learning curves / CV: check generalization stability.\n",
    "\n",
    "**Best practice**\n",
    "- Do cross-validation for stable estimates, especially with smaller datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce637b",
   "metadata": {},
   "source": [
    "## Q60) How does the K-nearest neighbors (KNN) algorithm make predictions?\n",
    "\n",
    "**k-Nearest Neighbors (kNN)** is a lazy, non-parametric method.\n",
    "\n",
    "How it works:\n",
    "1. Choose \\(k\\).\n",
    "2. For a query point, compute distance to training points (Euclidean, cosine, etc.).\n",
    "3. Pick the \\(k\\) nearest points.\n",
    "4. Predict:\n",
    "   - **classification:** majority vote (optionally distance-weighted)\n",
    "   - **regression:** mean/weighted mean of neighbors’ targets\n",
    "\n",
    "It relies on the idea that “similar points have similar labels.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c084fcf",
   "metadata": {},
   "source": [
    "## Q61) What is the curse of dimensionality, and how does it affect machine learning algorithms?\n",
    "\n",
    "The **curse of dimensionality** describes how data becomes sparse and harder to model as the number of features grows.\n",
    "\n",
    "What happens:\n",
    "- volume of space grows exponentially → need much more data to cover it\n",
    "- distances become less informative (kNN, k-means suffer)\n",
    "- models overfit more easily (many degrees of freedom)\n",
    "\n",
    "How to mitigate:\n",
    "- feature selection (remove irrelevant/redundant features)\n",
    "- dimensionality reduction (PCA/UMAP)\n",
    "- regularization (Ridge/Lasso)\n",
    "- get more data or better features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d1410",
   "metadata": {},
   "source": [
    "## Q62) What is feature scaling, and why is it important in machine learning?\n",
    "\n",
    "**Feature scaling** transforms features to comparable ranges (e.g., standardization or min-max scaling).\n",
    "\n",
    "Why important:\n",
    "- **Distance-based models** (kNN, k-means) depend heavily on scales.\n",
    "- **Gradient-based models** (logistic regression, neural nets) converge faster with scaled inputs.\n",
    "- **SVM** is sensitive to scaling, especially with RBF kernel.\n",
    "- **PCA** requires scaling to avoid domination by large-scale features.\n",
    "\n",
    "Tree-based models are less sensitive, but scaling still helps in mixed pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb220648",
   "metadata": {},
   "source": [
    "## Q63) How does the Naïve Bayes algorithm handle categorical features?\n",
    "\n",
    "Naïve Bayes handles categorical features by estimating class-conditional probabilities for each category.\n",
    "\n",
    "Example:\n",
    "- Feature = Color ∈ {Red, Blue, Green}\n",
    "- For each class \\(y\\), estimate \\(P(Color=Red|y)\\), \\(P(Color=Blue|y)\\), etc.\n",
    "\n",
    "To avoid zero probabilities for rare/unseen categories, apply **smoothing** (Laplace/add-α).\n",
    "For text features (many categories = words), Multinomial/Bernoulli NB is commonly used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad61a4",
   "metadata": {},
   "source": [
    "## Q64) Explain the concept of prior and posterior probabilities in Naïve Bayes.\n",
    "\n",
    "**Prior probability**: \\(P(y)\\)  \n",
    "- The probability of class \\(y\\) before seeing any features.\n",
    "- Example: if 70% emails are not spam, \\(P(y=\\text{not spam})=0.7\\).\n",
    "\n",
    "**Likelihood**: \\(P(x|y)\\)  \n",
    "- Probability of observing features \\(x\\) given class \\(y\\).\n",
    "\n",
    "**Posterior probability**: \\(P(y|x)\\)  \n",
    "- Updated probability after seeing features:\n",
    "\\[\n",
    "P(y|x) = \\frac{P(x|y)\\,P(y)}{P(x)}\n",
    "\\]\n",
    "In classification, we compare posteriors across classes and pick the largest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a100ade",
   "metadata": {},
   "source": [
    "## Q65) What is Laplace smoothing, and why is it used in Naïve Bayes?\n",
    "\n",
    "**Laplace smoothing** (add-one smoothing) prevents zero probabilities when a category/word is unseen in training for a class.\n",
    "\n",
    "Without smoothing:\n",
    "- If \\(P(x_j|y)=0\\) for any feature, the entire product becomes 0 → model fails.\n",
    "\n",
    "With Laplace smoothing:\n",
    "\\[\n",
    "P(w|c) = \\frac{count(w,c)+\\alpha}{\\sum_w count(w,c) + \\alpha V}\n",
    "\\]\n",
    "- \\(V\\) = vocabulary size (or number of categories)\n",
    "- \\(\\alpha=1\\) gives classic Laplace smoothing\n",
    "\n",
    "This improves robustness, especially in sparse text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec54aa",
   "metadata": {},
   "source": [
    "## Q66) Can Naïve Bayes handle continuous features?\n",
    "\n",
    "Yes. **Gaussian Naïve Bayes** is designed for continuous (real-valued) features.\n",
    "It assumes each feature is normally distributed within each class:\n",
    "\\[\n",
    "x_j|y \\sim \\mathcal{N}(\\mu_{jy}, \\sigma_{jy}^2)\n",
    "\\]\n",
    "It estimates \\(\\mu\\) and \\(\\sigma^2\\) from the training data per class and uses them to compute likelihoods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7c528",
   "metadata": {},
   "source": [
    "## Q67) What are the assumptions of the Naïve Bayes algorithm?\n",
    "\n",
    "Key assumptions of Naïve Bayes:\n",
    "1. **Conditional independence**: features are independent given the class.\n",
    "2. **Correct likelihood model** for features:\n",
    "   - Gaussian for continuous (GNB)\n",
    "   - Multinomial/Bernoulli/Categorical for discrete/text\n",
    "3. **i.i.d. samples**: training examples are independent and identically distributed.\n",
    "\n",
    "When these assumptions are approximately reasonable, NB can be very effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbab91a",
   "metadata": {},
   "source": [
    "## Q68) How does Naïve Bayes handle missing values?\n",
    "\n",
    "Most NB implementations cannot accept NaNs directly, so missing values are handled by preprocessing:\n",
    "- impute numeric features (mean/median)\n",
    "- impute categorical features (mode) or introduce a category like “Unknown”\n",
    "- add missingness indicators if missingness is informative\n",
    "\n",
    "Always fit imputers on the training set only, then apply the same transformation to validation/test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d38058",
   "metadata": {},
   "source": [
    "## Q69) What are some common applications of Naïve Bayes?\n",
    "\n",
    "Naïve Bayes is widely used in problems where features are many and sparse, especially text.\n",
    "\n",
    "Common applications:\n",
    "- **Spam detection**\n",
    "- **Sentiment analysis**\n",
    "- **Document/topic classification**\n",
    "- **Language detection**\n",
    "- **Medical diagnosis baselines** (quick baseline)\n",
    "- **Real-time classification** where speed matters\n",
    "\n",
    "It is often used as a strong baseline because it trains and predicts very fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6db21",
   "metadata": {},
   "source": [
    "## Q70) Explain the difference between generative and discriminative models.\n",
    "\n",
    "**Generative models** learn how data is generated by modeling \\(P(x,y)\\) or \\(P(x|y)\\) and \\(P(y)\\).\n",
    "- Example: Naïve Bayes (models \\(P(x|y)\\) and \\(P(y)\\)).\n",
    "- Can generate samples of \\(x\\) given \\(y\\) (in principle).\n",
    "\n",
    "**Discriminative models** directly model \\(P(y|x)\\) or learn the decision boundary.\n",
    "- Examples: logistic regression, SVM, neural networks.\n",
    "- Often achieve higher predictive accuracy when enough labeled data exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2a7d9",
   "metadata": {},
   "source": [
    "## Q71) How does the decision boundary of a Naïve Bayes classifier look like for binary classification tasks?\n",
    "\n",
    "The Naïve Bayes decision boundary depends on the NB variant:\n",
    "\n",
    "- For **Multinomial/Bernoulli NB**, taking logs yields a linear function of features in many common representations (e.g., bag-of-words), so the boundary is often **approximately linear** in that feature space.\n",
    "- For **Gaussian NB**, if each class has **equal variances** per feature, the boundary becomes **linear** (similar to LDA). If variances differ, the boundary can be **quadratic** (curved).\n",
    "\n",
    "So: linear in many cases, quadratic in general for GNB with unequal variances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d1d48",
   "metadata": {},
   "source": [
    "## Q72) What is the difference between multinomial Naïve Bayes and Gaussian Naïve Bayes?\n",
    "\n",
    "**Multinomial Naïve Bayes**\n",
    "- Designed for **count data**: word counts, term frequencies.\n",
    "- Likelihood is multinomial.\n",
    "- Common in text classification (spam, sentiment).\n",
    "\n",
    "**Gaussian Naïve Bayes**\n",
    "- Designed for **continuous** features.\n",
    "- Assumes normal distribution per feature per class.\n",
    "- Used for numeric datasets where Gaussian assumption is acceptable.\n",
    "\n",
    "Choose based on data type and representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b303f",
   "metadata": {},
   "source": [
    "## Q73) How does Naïve Bayes handle numerical instability issues?\n",
    "\n",
    "Naïve Bayes can face numerical underflow because it multiplies many small probabilities:\n",
    "\\[\n",
    "P(y|x) \\propto P(y)\\prod_j P(x_j|y)\n",
    "\\]\n",
    "To fix this, implementations use **log-space**:\n",
    "\\[\n",
    "\\log P(y|x) = \\log P(y) + \\sum_j \\log P(x_j|y) + C\n",
    "\\]\n",
    "Additionally, smoothing ensures probabilities are never exactly zero, avoiding \\(\\log(0)\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68304da",
   "metadata": {},
   "source": [
    "## Q74) What is the Laplacian correction, and when is it used in Naïve Bayes?\n",
    "\n",
    "**Laplacian correction** is another name for **Laplace (add-one) smoothing**.\n",
    "It is used when estimating probabilities from counts for categorical/text features to:\n",
    "- avoid zero probabilities for unseen events\n",
    "- improve generalization on sparse datasets\n",
    "\n",
    "General add-α version:\n",
    "\\[\n",
    "P = \\frac{count+\\alpha}{N+\\alpha V}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b61b5",
   "metadata": {},
   "source": [
    "## Q75) Can Naïve Bayes be used for regression tasks?\n",
    "\n",
    "In general coursework and common ML practice, Naïve Bayes is not used for regression.\n",
    "Regression requires predicting continuous values, and NB is built around computing class posteriors.\n",
    "\n",
    "For regression, typical models include:\n",
    "- linear regression / Ridge / Lasso\n",
    "- SVR\n",
    "- tree ensembles (Random Forest, Gradient Boosting)\n",
    "- neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef8779",
   "metadata": {},
   "source": [
    "## Q76) Explain the concept of conditional independence assumption in Naïve Bayes.\n",
    "\n",
    "**Conditional independence** in Naïve Bayes means:\n",
    "Given the class \\(y\\), the features do not depend on each other:\n",
    "\\[\n",
    "P(x_1,\\dots,x_p|y) = \\prod_{j=1}^p P(x_j|y)\n",
    "\\]\n",
    "This assumption dramatically simplifies learning:\n",
    "- Instead of estimating a full joint distribution over features, NB estimates each feature distribution separately per class.\n",
    "Even when not perfectly true, NB often performs well, especially with many weakly informative features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1eb7a6",
   "metadata": {},
   "source": [
    "## Q77) How does Naïve Bayes handle categorical features with a large number of categories?\n",
    "\n",
    "For categorical features with many categories:\n",
    "- probability estimates become unreliable for rare categories\n",
    "- one-hot can create very sparse/high-dimensional features\n",
    "\n",
    "Common handling:\n",
    "- **Laplace/add-α smoothing**\n",
    "- **group rare categories** into “Other”\n",
    "- **feature hashing** (especially in text pipelines)\n",
    "- use text-style representations (counts/TF-IDF) if categories behave like tokens\n",
    "\n",
    "Goal: stabilize probabilities and reduce dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083dedc",
   "metadata": {},
   "source": [
    "## Q78) What are some drawbacks of the Naïve Bayes algorithm?\n",
    "\n",
    "Drawbacks of Naïve Bayes:\n",
    "- independence assumption often violated → can limit accuracy\n",
    "- probability estimates can be poorly calibrated (overconfident)\n",
    "- Gaussian NB can fail when features are not close to normal per class\n",
    "- cannot capture complex feature interactions without feature engineering\n",
    "- sensitive to representation (e.g., for text: stopwords, tokenization choices matter)\n",
    "\n",
    "Still, NB is a strong, fast baseline in many classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d02af6",
   "metadata": {},
   "source": [
    "## Q79) Explain the concept of smoothing in Naïve Bayes.\n",
    "\n",
    "**Smoothing** in Naïve Bayes adjusts probability estimates to avoid zeros and reduce overfitting to sparse counts.\n",
    "\n",
    "Why needed:\n",
    "- In sparse data, some events may not appear in training but can appear in test.\n",
    "- Without smoothing, unseen events have probability 0 → kills the entire posterior.\n",
    "\n",
    "Common smoothing:\n",
    "- Laplace (add-one): \\(\\alpha=1\\)\n",
    "- Add-α smoothing: \\(\\alpha\\) tuned (e.g., 0.1, 0.5, 1.0) for best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550f73d",
   "metadata": {},
   "source": [
    "## Q80) How does Naïve Bayes handle imbalanced datasets?\n",
    "\n",
    "Naïve Bayes can be affected by imbalanced classes because the **prior** \\(P(y)\\) may dominate decisions.\n",
    "\n",
    "How NB deals with imbalance:\n",
    "- **Class priors**: it naturally uses \\(P(y)\\), reflecting imbalance.\n",
    "- **Threshold tuning**: instead of 0.5, choose a threshold that improves recall/precision.\n",
    "- **Evaluation**: use F1, recall, PR-AUC (not only accuracy).\n",
    "\n",
    "Additional strategies (outside NB itself):\n",
    "- resampling training data (oversample/undersample, SMOTE)\n",
    "- cost-sensitive learning in other models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc6d5d",
   "metadata": {},
   "source": [
    "---\n",
    "# Optional Demo Code (supports multiple questions)\n",
    "These cells are optional and demonstrate regression metrics, logistic regression metrics,\n",
    "decision trees, SVM, Naïve Bayes smoothing, and cross-validation/grid search.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de665be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If scikit-learn is available, these demos will run.\n",
    "# If not, you can install it locally with: pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d0f5b",
   "metadata": {},
   "source": [
    "## Demo A) Regression metrics (MAE, RMSE, R²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc509305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE : 10.70647762693496\n",
      "RMSE: 13.746574623072956\n",
      "R^2 : 0.987065779228942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.datasets import make_regression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "    X, y = make_regression(n_samples=600, n_features=6, noise=15.0, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    lr = LinearRegression().fit(X_train, y_train)\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(\"MAE :\", mean_absolute_error(y_test, pred))\n",
    "    print(\"RMSE:\", mean_squared_error(y_test, pred, squared=False))\n",
    "    print(\"R^2 :\", r2_score(y_test, pred))\n",
    "except Exception as e:\n",
    "    print(\"Demo skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc59ad",
   "metadata": {},
   "source": [
    "## Demo B) Logistic regression evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904c433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[412  12]\n",
      " [ 26  50]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       424\n",
      "           1       0.81      0.66      0.72        76\n",
      "\n",
      "    accuracy                           0.92       500\n",
      "   macro avg       0.87      0.81      0.84       500\n",
      "weighted avg       0.92      0.92      0.92       500\n",
      "\n",
      "ROC-AUC: 0.920990566037736\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "    Xc, yc = make_classification(n_samples=2000, n_features=10, n_informative=6,\n",
    "                                 weights=[0.85, 0.15], random_state=42)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.25, stratify=yc, random_state=42)\n",
    "\n",
    "    clf = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                    (\"lr\", LogisticRegression(max_iter=2000))])\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    proba = clf.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, proba))\n",
    "except Exception as e:\n",
    "    print(\"Demo skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e7fe1",
   "metadata": {},
   "source": [
    "## Demo C) Naïve Bayes smoothing effect (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16287234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance (train): Counter({0: 889, 1: 236})\n",
      "F1 (alpha=0): 0.0\n",
      "F1 (alpha=1): 0.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "    X_nb = rng.poisson(lam=2.0, size=(1500, 25))  # count-like features\n",
    "    y_nb = rng.choice([0,1], size=1500, p=[0.8, 0.2])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_nb, y_nb, test_size=0.25, stratify=y_nb, random_state=42)\n",
    "    print(\"Class balance (train):\", Counter(y_train))\n",
    "\n",
    "    nb0 = MultinomialNB(alpha=0.0).fit(X_train, y_train)\n",
    "    nb1 = MultinomialNB(alpha=1.0).fit(X_train, y_train)\n",
    "\n",
    "    pred0 = nb0.predict(X_test)\n",
    "    pred1 = nb1.predict(X_test)\n",
    "\n",
    "    print(\"F1 (alpha=0):\", f1_score(y_test, pred0))\n",
    "    print(\"F1 (alpha=1):\", f1_score(y_test, pred1))\n",
    "except Exception as e:\n",
    "    print(\"Demo skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69915359",
   "metadata": {},
   "source": [
    "## Demo D) Cross-validation + Grid Search (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c22f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'svm__C': 5, 'svm__gamma': 'scale'}\n",
      "Best CV F1: 0.8662329437781132\n",
      "Test accuracy: 0.9386666666666666\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    Xc, yc = make_classification(n_samples=1500, n_features=12, n_informative=7, weights=[0.8,0.2], random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xc, yc, test_size=0.25, stratify=yc, random_state=42)\n",
    "\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                     (\"svm\", SVC(kernel=\"rbf\"))])\n",
    "\n",
    "    grid = GridSearchCV(pipe, {\n",
    "        \"svm__C\": [0.5, 1, 5, 10],\n",
    "        \"svm__gamma\": [\"scale\", 0.1, 0.01]\n",
    "    }, cv=5, scoring=\"f1\", n_jobs=-1)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"Best params:\", grid.best_params_)\n",
    "    print(\"Best CV F1:\", grid.best_score_)\n",
    "    print(\"Test accuracy:\", accuracy_score(y_test, grid.best_estimator_.predict(X_test)))\n",
    "except Exception as e:\n",
    "    print(\"Demo skipped:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
