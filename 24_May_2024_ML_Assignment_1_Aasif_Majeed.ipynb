{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce75f52",
   "metadata": {},
   "source": [
    "# ML Assignment – 1  \n",
    "**Submitter Name:** Aasif Majeed  \n",
    "**Date:** 24 May 2024  \n",
    "\n",
    "This notebook contains **all questions (1–80)** from the ML Assignment PDF and provides clear, exam-ready answers.  \n",
    "Where helpful, short **Python demo code** is included for key concepts (splits, scaling, SMOTE, interpolation, PCA, encoding, VIF, RFE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad98ab",
   "metadata": {},
   "source": [
    "---\n",
    "## 0) Python Setup (for demo code)\n",
    "> The answers are mostly theoretical. The code below is only for *demonstration*.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbde9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 140)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb4e8b",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo Dataset (Synthetic) for code examples\n",
    "We create a small synthetic dataset to demonstrate:\n",
    "- train/validation/test split\n",
    "- scaling and PCA\n",
    "- encoding\n",
    "- imbalance handling (SMOTE)\n",
    "- outlier detection\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcb60d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 12), array([1833,  167]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification, make_regression\n",
    "\n",
    "# Classification dataset with imbalance (for SMOTE + metrics)\n",
    "Xc, yc = make_classification(\n",
    "    n_samples=2000, n_features=12, n_informative=6, n_redundant=2,\n",
    "    weights=[0.92, 0.08], flip_y=0.01, random_state=42\n",
    ")\n",
    "\n",
    "# Regression dataset (for scaling + PCA + feature selection demos)\n",
    "Xr, yr = make_regression(\n",
    "    n_samples=800, n_features=10, n_informative=6, noise=15.0, random_state=42\n",
    ")\n",
    "\n",
    "Xc.shape, np.bincount(yc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8a17d",
   "metadata": {},
   "source": [
    "---\n",
    "# Answers (1–80)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557d3b5",
   "metadata": {},
   "source": [
    "## Q1) Define Artificial Intelligence (AI).\n",
    "\n",
    "**Artificial Intelligence (AI)** is a broad field of computer science focused on building systems that can perform tasks that normally require human intelligence, such as:\n",
    "- perception (vision/speech),\n",
    "- reasoning and decision-making,\n",
    "- learning from data/experience,\n",
    "- language understanding and generation,\n",
    "- planning and acting in environments.\n",
    "\n",
    "In short: **AI aims to create “intelligent behavior” in machines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2366ed0",
   "metadata": {},
   "source": [
    "## Q2) Explain the differences between Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Data Science (DS).\n",
    "\n",
    "- **AI (Artificial Intelligence):** Umbrella field—any approach that makes machines behave intelligently (rule-based, search, ML, etc.).  \n",
    "- **ML (Machine Learning):** Subset of AI where models *learn patterns from data* instead of being explicitly programmed.  \n",
    "- **DL (Deep Learning):** Subset of ML using multi-layer neural networks; excellent for images, audio, and text at large scale.  \n",
    "- **DS (Data Science):** A broader discipline focused on extracting insight/value from data using statistics, ML, visualization, and domain knowledge (often includes building dashboards, experiments, business decisions, etc.).\n",
    "\n",
    "**Relationship:** AI ⊃ ML ⊃ DL, while DS overlaps with ML/DL but also includes analytics, experimentation, and reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce7f2a",
   "metadata": {},
   "source": [
    "## Q3) How does AI differ from traditional software development?\n",
    "\n",
    "Traditional software uses **explicit rules**: “if X then do Y”.  \n",
    "AI systems often use **models** that learn behavior from data or experience.\n",
    "\n",
    "Key differences:\n",
    "- **Rule-based vs data-driven:** Traditional = fixed logic; AI/ML = learns patterns from data.\n",
    "- **Deterministic vs probabilistic:** Traditional outputs are predictable; AI outputs may be probabilistic.\n",
    "- **Maintenance:** Traditional changes need code updates; AI changes may need re-training / new data.\n",
    "- **Testing:** Traditional tests logic branches; AI tests generalization with validation/test data.\n",
    "- **Performance:** Traditional improves via better code; AI improves via better data, features, model, training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c1162",
   "metadata": {},
   "source": [
    "## Q4) Provide examples of AI, ML, DL, and DS applications.\n",
    "\n",
    "**Examples**\n",
    "- **AI:** chess/Go engine, voice assistants, self-driving planning, expert systems.\n",
    "- **ML:** spam detection, house price prediction, credit scoring, recommendation systems.\n",
    "- **DL:** image classification, speech-to-text, large language models, medical image segmentation.\n",
    "- **DS:** customer churn analysis, A/B testing, KPI dashboards, forecasting demand, root-cause analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698b624",
   "metadata": {},
   "source": [
    "## Q5) Discuss the importance of AI, ML, DL, and DS in today's world.\n",
    "\n",
    "Why these fields matter today:\n",
    "- **Automation & productivity:** automating repetitive tasks (customer support, document processing).\n",
    "- **Better decisions:** predictive analytics (risk, churn, demand forecasting).\n",
    "- **Personalization:** recommendations in e-commerce/streaming.\n",
    "- **Healthcare:** diagnosis assistance, drug discovery, monitoring.\n",
    "- **Safety & security:** anomaly detection, fraud detection, cybersecurity.\n",
    "- **Research acceleration:** faster discovery in engineering, materials, climate, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb2167",
   "metadata": {},
   "source": [
    "## Q6) What is Supervised Learning?\n",
    "\n",
    "**Supervised Learning** is an ML setting where we learn a mapping **X → y** using labeled examples:\n",
    "- Inputs/features: **X**\n",
    "- Target/label: **y**\n",
    "Goal: learn a model that predicts **y** for unseen **X**.\n",
    "\n",
    "Two main types:\n",
    "- **Classification:** y is categorical (spam/not spam)\n",
    "- **Regression:** y is continuous (price, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaedc9f",
   "metadata": {},
   "source": [
    "## Q7) Provide examples of Supervised Learning algorithms.\n",
    "\n",
    "Examples of supervised algorithms:\n",
    "- **Linear Regression**, **Ridge/Lasso** (regression)\n",
    "- **Logistic Regression** (classification)\n",
    "- **Decision Trees**, **Random Forest**\n",
    "- **Gradient Boosting** (XGBoost/LightGBM/CatBoost)\n",
    "- **Support Vector Machines (SVM)**\n",
    "- **k-Nearest Neighbors (kNN)**\n",
    "- **Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c4d66",
   "metadata": {},
   "source": [
    "## Q8) Explain the process of Supervised Learning.\n",
    "\n",
    "Typical supervised learning workflow:\n",
    "1. **Collect labeled data** (X, y)\n",
    "2. **Clean/prepare data** (missing values, encoding, scaling)\n",
    "3. **Split data** into train/validation/test\n",
    "4. **Train** model on training set\n",
    "5. **Tune** hyperparameters using validation (or cross-validation)\n",
    "6. **Evaluate** on test set (final unbiased estimate)\n",
    "7. **Deploy** and **monitor** (data drift, performance decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a140e55",
   "metadata": {},
   "source": [
    "## Q9) What are the characteristics of Unsupervised Learning?\n",
    "\n",
    "**Unsupervised Learning** uses **unlabeled** data (only X). Common characteristics:\n",
    "- Goal is to **discover structure/patterns** in data\n",
    "- Outputs are not “correct labels” but groupings/representations\n",
    "- Often used for **clustering**, **dimensionality reduction**, **density estimation**, **anomaly detection**\n",
    "- Evaluation is harder (no true labels); uses internal metrics or downstream task performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005e845",
   "metadata": {},
   "source": [
    "## Q10) Give examples of Unsupervised Learning algorithms.\n",
    "\n",
    "Examples of unsupervised algorithms:\n",
    "- **K-Means**, **Hierarchical Clustering**, **DBSCAN**\n",
    "- **Gaussian Mixture Models (GMM)**\n",
    "- **PCA**, **t-SNE**, **UMAP** (dimensionality reduction/visualization)\n",
    "- **Association rules** (Apriori)\n",
    "- **Autoencoders** (representation learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c09b3d",
   "metadata": {},
   "source": [
    "## Q11) Describe Semi-Supervised Learning and its significance.\n",
    "\n",
    "**Semi-Supervised Learning** uses:\n",
    "- a small labeled dataset + a large unlabeled dataset.\n",
    "\n",
    "Why important:\n",
    "- Labels can be expensive (medical data, manual annotation)\n",
    "- Uses unlabeled data to improve generalization\n",
    "Examples:\n",
    "- pseudo-labeling, consistency regularization, graph-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24319e",
   "metadata": {},
   "source": [
    "## Q12) Explain Reinforcement Learning and its applications.\n",
    "\n",
    "**Reinforcement Learning (RL)** is learning by interaction:\n",
    "- an **agent** takes actions in an **environment**\n",
    "- receives **reward** and new **state**\n",
    "- objective: maximize long-term cumulative reward.\n",
    "\n",
    "Applications:\n",
    "- games (Atari, chess/Go)\n",
    "- robotics control\n",
    "- recommendation policies / ads bidding\n",
    "- scheduling, resource allocation, operations research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7331164",
   "metadata": {},
   "source": [
    "## Q13) How does Reinforcement Learning differ from Supervised and Unsupervised Learning?\n",
    "\n",
    "Differences:\n",
    "- **Supervised:** learn from labeled (X, y) pairs; feedback is direct and immediate.\n",
    "- **Unsupervised:** learn patterns/structure from X only; no labels.\n",
    "- **RL:** learn by trial-and-error with **rewards**, where feedback can be delayed; must explore/exploit and handle sequential decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca9562",
   "metadata": {},
   "source": [
    "## Q14) What is the purpose of the Train-Test-Validation split in machine learning?\n",
    "\n",
    "Train/Test/Validation split is used to:\n",
    "- **Train**: fit model parameters\n",
    "- **Validation**: tune hyperparameters, select model, early stopping\n",
    "- **Test**: final unbiased estimate of generalization performance\n",
    "\n",
    "Purpose: avoid **overfitting** to the same data used for development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42df234",
   "metadata": {},
   "source": [
    "## Q15) Explain the significance of the training set.\n",
    "\n",
    "The training set:\n",
    "- is the data the model learns from (fits weights/parameters)\n",
    "- must be representative of real-world data\n",
    "- quality and quantity of training data strongly affect generalization\n",
    "\n",
    "Bad training data (biased/noisy) → bad model, regardless of algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b44c79",
   "metadata": {},
   "source": [
    "## Q16) How do you determine the size of the training, testing, and validation sets?\n",
    "\n",
    "How to choose split sizes:\n",
    "- depends on dataset size, model complexity, and variance of evaluation\n",
    "Common choices:\n",
    "- **70/15/15**, **80/10/10**, **80/20** (no validation if using CV)\n",
    "Rules of thumb:\n",
    "- If data is large → smaller test is OK\n",
    "- If data is small → use **k-fold cross-validation**\n",
    "- For time-series → use time-based split (no shuffling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cff5e",
   "metadata": {},
   "source": [
    "## Q17) What are the consequences of improper Train-Test-Validation splits?\n",
    "\n",
    "Improper splits can cause:\n",
    "- **data leakage** (train sees info from test/validation)\n",
    "- overly optimistic results (model seems better than it is)\n",
    "- poor real-world performance after deployment\n",
    "- unstable evaluation due to too-small test set\n",
    "- wrong model selection/hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17adc367",
   "metadata": {},
   "source": [
    "## Q18) Discuss the trade-offs in selecting appropriate split ratios.\n",
    "\n",
    "Trade-offs:\n",
    "- More training data → better learning, but less reliable evaluation (small test)\n",
    "- Larger test/validation → more reliable estimate, but less training data\n",
    "- For high-stakes tasks → prefer larger test set or repeated CV\n",
    "- For imbalanced/time-series/grouped data → use stratified/time-based/group splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa22574",
   "metadata": {},
   "source": [
    "## Q19) Define model performance in machine learning.\n",
    "\n",
    "**Model performance** = how well a model meets objectives on unseen data, such as:\n",
    "- predictive accuracy / error\n",
    "- calibration (probability quality)\n",
    "- robustness (noise/outliers)\n",
    "- fairness and stability\n",
    "- latency/memory constraints (for deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16441a6f",
   "metadata": {},
   "source": [
    "## Q20) How do you measure the performance of a machine learning model?\n",
    "\n",
    "Measurement depends on task:\n",
    "\n",
    "**Classification metrics**\n",
    "- Accuracy, Precision, Recall, F1-score\n",
    "- ROC-AUC, PR-AUC (for imbalance)\n",
    "- Confusion matrix\n",
    "\n",
    "**Regression metrics**\n",
    "- MAE, MSE, RMSE\n",
    "- R²\n",
    "\n",
    "Also evaluate:\n",
    "- Cross-validation scores\n",
    "- Learning curves\n",
    "- Business metrics (cost, profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190197c",
   "metadata": {},
   "source": [
    "## Q21) What is overfitting and why is it problematic?\n",
    "\n",
    "**Overfitting** happens when a model learns noise and training-specific patterns, performing well on training data but poorly on unseen data.\n",
    "\n",
    "Why problematic:\n",
    "- poor generalization\n",
    "- unreliable predictions in real world\n",
    "- often caused by high model complexity, small data, leakage, or too much training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0238f4a",
   "metadata": {},
   "source": [
    "## Q22) Provide techniques to address overfitting.\n",
    "\n",
    "Techniques to reduce overfitting:\n",
    "- More data / data augmentation\n",
    "- Regularization (L1/L2, dropout)\n",
    "- Simpler model (reduce complexity)\n",
    "- Early stopping\n",
    "- Cross-validation\n",
    "- Feature selection / dimensionality reduction\n",
    "- Ensembling (bagging), pruning trees\n",
    "- Proper train/val/test separation (avoid leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037964c1",
   "metadata": {},
   "source": [
    "## Q23) Explain underfitting and its implications.\n",
    "\n",
    "**Underfitting** occurs when a model is too simple to capture the underlying pattern.\n",
    "Symptoms:\n",
    "- high error on training data and test data\n",
    "Implications:\n",
    "- model cannot learn important structure → poor predictions everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b06d4",
   "metadata": {},
   "source": [
    "## Q24) How can you prevent underfitting in machine learning models?\n",
    "\n",
    "Prevent underfitting by:\n",
    "- using a more expressive model (nonlinear, deeper, more features)\n",
    "- reducing regularization if too strong\n",
    "- training longer / better optimization\n",
    "- adding useful features (feature engineering)\n",
    "- reducing noise via better preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69998a",
   "metadata": {},
   "source": [
    "## Q25) Discuss the balance between bias and variance in model performance.\n",
    "\n",
    "**Bias–Variance trade-off**\n",
    "- **Bias:** error from oversimplified assumptions → underfitting\n",
    "- **Variance:** error from sensitivity to training data → overfitting\n",
    "\n",
    "Goal: choose model/regularization that balances both, minimizing test error.\n",
    "Tools:\n",
    "- learning curves\n",
    "- cross-validation\n",
    "- regularization and model complexity control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b452d",
   "metadata": {},
   "source": [
    "## Q26) What are the common techniques to handle missing data?\n",
    "\n",
    "Common ways to handle missing data:\n",
    "- **Remove** rows/columns (if few missing and safe)\n",
    "- **Simple imputation:** mean/median (numeric), mode (categorical)\n",
    "- **Advanced imputation:** KNN imputer, iterative/multivariate imputation\n",
    "- **Model-based handling:** some models handle missing (e.g., XGBoost can)\n",
    "- **Add indicator features** (missingness flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125162e",
   "metadata": {},
   "source": [
    "## Q27) Explain the implications of ignoring missing data.\n",
    "\n",
    "Ignoring missing data can:\n",
    "- reduce dataset size (if rows dropped automatically)\n",
    "- bias results if missingness is not random\n",
    "- break algorithms (many models cannot handle NaNs)\n",
    "- distort feature distributions → wrong relationships and wrong predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c1baa",
   "metadata": {},
   "source": [
    "## Q28) Discuss the pros and cons of imputation methods.\n",
    "\n",
    "**Imputation pros**\n",
    "- keeps more data (better statistical power)\n",
    "- allows models requiring complete data to run\n",
    "- can reduce bias vs dropping rows (depending on missing mechanism)\n",
    "\n",
    "**Imputation cons**\n",
    "- can introduce bias if imputation is unrealistic\n",
    "- underestimates uncertainty (treats filled values as true)\n",
    "- complex methods may be slow and may leak information if fit on full dataset (should fit only on training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417af70e",
   "metadata": {},
   "source": [
    "## Q29) How does missing data affect model performance?\n",
    "\n",
    "Missing data affects performance by:\n",
    "- reducing effective training size\n",
    "- increasing noise/bias in estimated relationships\n",
    "- causing models to learn wrong patterns if missingness is correlated with target\n",
    "- producing unpredictable inference if missing patterns differ between train and test (missingness drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed924c",
   "metadata": {},
   "source": [
    "## Q30) Define imbalanced data in the context of machine learning.\n",
    "\n",
    "**Imbalanced data** means class frequencies are very unequal (e.g., 95% class 0, 5% class 1).\n",
    "Common in fraud detection, rare disease diagnosis, defect detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f94cf0",
   "metadata": {},
   "source": [
    "## Q31) Discuss the challenges posed by imbalanced data.\n",
    "\n",
    "Challenges of imbalanced data:\n",
    "- accuracy becomes misleading (predict majority class and get high accuracy)\n",
    "- minority class recall/precision often poor\n",
    "- decision boundary biased toward majority class\n",
    "- model may not learn minority patterns due to few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649cc46",
   "metadata": {},
   "source": [
    "## Q32) What techniques can be used to address imbalanced data?\n",
    "\n",
    "Techniques for imbalance:\n",
    "- **Resampling:** oversampling minority, undersampling majority\n",
    "- **SMOTE/ADASYN** synthetic sampling\n",
    "- **Class weights / cost-sensitive learning**\n",
    "- **Threshold tuning**\n",
    "- **Use better metrics:** PR-AUC, F1, recall, balanced accuracy\n",
    "- **Ensembles:** Balanced Random Forest, EasyEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d254a3f",
   "metadata": {},
   "source": [
    "## Q33) Explain the process of up-sampling and down-sampling.\n",
    "\n",
    "**Up-sampling (oversampling):**\n",
    "- increase minority examples by duplicating or synthesizing new ones\n",
    "\n",
    "**Down-sampling (undersampling):**\n",
    "- reduce majority examples by removing samples\n",
    "\n",
    "Workflow:\n",
    "1. Split train/test first (avoid leakage)\n",
    "2. Apply resampling on training set only\n",
    "3. Train model, evaluate on untouched test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548004e5",
   "metadata": {},
   "source": [
    "## Q34) When would you use up-sampling versus down-sampling?\n",
    "\n",
    "Use **up-sampling** when:\n",
    "- dataset is small and you cannot afford removing majority data\n",
    "- minority class is very rare and important\n",
    "\n",
    "Use **down-sampling** when:\n",
    "- dataset is huge (majority class has many redundant samples)\n",
    "- training time is an issue\n",
    "- you can remove some majority examples without losing important information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f927ac",
   "metadata": {},
   "source": [
    "## Q35) What is SMOTE and how does it work?\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)** creates synthetic minority points by:\n",
    "1. for each minority sample, find k nearest minority neighbors\n",
    "2. pick a neighbor and create a synthetic point along the line segment between them\n",
    "\n",
    "This increases minority diversity without exact duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35af66",
   "metadata": {},
   "source": [
    "## Q36) Explain the role of SMOTE in handling imbalanced data.\n",
    "\n",
    "Role of SMOTE:\n",
    "- balances training data so classifier learns minority patterns better\n",
    "- often improves recall/F1 for minority class\n",
    "- should be applied **only on training split** to avoid leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb705ad4",
   "metadata": {},
   "source": [
    "## Q37) Discuss the advantages and limitations of SMOTE.\n",
    "\n",
    "**Advantages**\n",
    "- reduces overfitting compared to simple duplication\n",
    "- improves minority decision region learning\n",
    "\n",
    "**Limitations**\n",
    "- can create overlapping classes (more false positives)\n",
    "- sensitive to noise/outliers (may generate bad synthetic points)\n",
    "- does not respect complex manifolds; may be unrealistic for some domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b564d",
   "metadata": {},
   "source": [
    "## Q38) Provide examples of scenarios where SMOTE is beneficial.\n",
    "\n",
    "SMOTE is useful when:\n",
    "- minority class has too few samples (fraud, rare events)\n",
    "- classes are reasonably separable but minority boundary is under-learned\n",
    "Examples:\n",
    "- credit card fraud detection\n",
    "- manufacturing defect detection\n",
    "- medical diagnosis with rare positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267e2ec",
   "metadata": {},
   "source": [
    "## Q39) Define data interpolation and its purpose.\n",
    "\n",
    "**Data interpolation** estimates missing values between known data points.\n",
    "Purpose:\n",
    "- fill missing values in time series or continuous measurements\n",
    "- create smoother signals (e.g., sensor data)\n",
    "- align data to common sampling rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd1893",
   "metadata": {},
   "source": [
    "## Q40) What are the common methods of data interpolation?\n",
    "\n",
    "Common interpolation methods:\n",
    "- **Linear interpolation**\n",
    "- **Polynomial interpolation**\n",
    "- **Spline interpolation** (cubic splines)\n",
    "- **Nearest neighbor interpolation**\n",
    "- **Time-series methods** (forward/backward fill; not true interpolation but often used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe07b4",
   "metadata": {},
   "source": [
    "## Q41) Discuss the implications of using data interpolation in machine learning.\n",
    "\n",
    "Implications of interpolation:\n",
    "- can improve models if missingness is small and data is smooth\n",
    "- can introduce bias (fabricated values) if behavior is nonlinear or abrupt\n",
    "- may leak future information in time series if not careful (e.g., using future points to fill past)\n",
    "Best practice:\n",
    "- apply interpolation only with correct time direction and only on training set for modeling pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce76340",
   "metadata": {},
   "source": [
    "## Q42) What are outliers in a dataset?\n",
    "\n",
    "**Outliers** are data points that are unusually far from the majority of observations.\n",
    "They may occur due to:\n",
    "- measurement errors\n",
    "- rare events\n",
    "- data entry issues\n",
    "- true but extreme behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1184f",
   "metadata": {},
   "source": [
    "## Q43) Explain the impact of outliers on machine learning models.\n",
    "\n",
    "Impact of outliers:\n",
    "- can distort mean/variance and correlation\n",
    "- can heavily affect models sensitive to scale (linear regression, kNN, SVM)\n",
    "- can cause unstable decision boundaries\n",
    "- may reduce model accuracy and generalization if they are noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8b566",
   "metadata": {},
   "source": [
    "## Q44) Discuss techniques for identifying outliers.\n",
    "\n",
    "Outlier detection techniques:\n",
    "- **Z-score** / standard deviation rule\n",
    "- **IQR method** (boxplot fences)\n",
    "- **Isolation Forest**\n",
    "- **Local Outlier Factor (LOF)**\n",
    "- **DBSCAN** (points in low-density region)\n",
    "- visualization: scatter plots, box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8f74f",
   "metadata": {},
   "source": [
    "## Q45) How can outliers be handled in a dataset?\n",
    "\n",
    "Handling outliers:\n",
    "- verify and correct data entry errors\n",
    "- remove outliers (only if they are incorrect/noise)\n",
    "- cap/winsorize (clip to percentile bounds)\n",
    "- transform features (log/Box-Cox)\n",
    "- use robust models/metrics (Huber loss, median-based methods)\n",
    "- separate rare-event class if outliers represent a real phenomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f19a98",
   "metadata": {},
   "source": [
    "## Q46) Compare and contrast Filter, Wrapper, and Embedded methods for feature selection.\n",
    "\n",
    "**Filter methods:** select features using statistics independent of model (fast).\n",
    "- e.g., correlation, chi-square, mutual information.\n",
    "\n",
    "**Wrapper methods:** use a model to evaluate feature subsets (more accurate but expensive).\n",
    "- e.g., forward selection, backward elimination, recursive feature elimination (RFE).\n",
    "\n",
    "**Embedded methods:** feature selection happens inside model training.\n",
    "- e.g., Lasso (L1), tree-based feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd52ebd",
   "metadata": {},
   "source": [
    "## Q47) Provide examples of algorithms associated with each method.\n",
    "\n",
    "Examples:\n",
    "- **Filter:** Pearson correlation filter, Chi-square test, ANOVA F-test, mutual information.\n",
    "- **Wrapper:** RFE with Logistic Regression/SVM, forward/backward selection using validation score.\n",
    "- **Embedded:** Lasso/ElasticNet, Decision Trees/Random Forest feature importance, Gradient Boosting importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905fd33",
   "metadata": {},
   "source": [
    "## Q48) Discuss the advantages and disadvantages of each feature selection method.\n",
    "\n",
    "Pros/cons:\n",
    "- **Filter**\n",
    "  - ✅ fast, simple, works well for initial screening\n",
    "  - ❌ may ignore feature interactions with the model\n",
    "- **Wrapper**\n",
    "  - ✅ can capture interactions, often better subset quality\n",
    "  - ❌ computationally expensive; risk of overfitting to validation\n",
    "- **Embedded**\n",
    "  - ✅ good balance (selection during training), efficient\n",
    "  - ❌ selection depends on model assumptions (e.g., Lasso for linear relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a1756",
   "metadata": {},
   "source": [
    "## Q49) Explain the concept of feature scaling.\n",
    "\n",
    "**Feature scaling** transforms numeric features to comparable ranges.\n",
    "Why needed:\n",
    "- distance-based models (kNN, K-means) depend on scale\n",
    "- gradient-based optimization (neural nets) converges faster\n",
    "- regularization depends on feature magnitude\n",
    "Not always necessary for trees (Decision Trees/Random Forest) but still can help in some pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e8757",
   "metadata": {},
   "source": [
    "## Q50) Describe the process of standardization.\n",
    "\n",
    "**Standardization (z-score scaling):**\n",
    "\\[ z = \\frac{x - \\mu}{\\sigma} \\]\n",
    "- mean becomes 0\n",
    "- standard deviation becomes 1\n",
    "Useful for many ML models (SVM, Logistic Regression, PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6168e",
   "metadata": {},
   "source": [
    "## Q51) How does mean normalization differ from standardization?\n",
    "\n",
    "**Mean normalization** typically:\n",
    "\\[ x' = \\frac{x - \\mu}{\\max(x)-\\min(x)} \\]\n",
    "or sometimes \\( (x - mean)/range \\)\n",
    "\n",
    "**Standardization**:\n",
    "\\[ x' = \\frac{x - \\mu}{\\sigma} \\]\n",
    "\n",
    "Difference:\n",
    "- mean normalization uses **range**; standardization uses **std dev**\n",
    "- standardization is more common for ML pipelines and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b3bab",
   "metadata": {},
   "source": [
    "## Q52) Discuss the advantages and disadvantages of Min-Max scaling.\n",
    "\n",
    "**Min-Max scaling**:\n",
    "\\[ x' = \\frac{x - x_{min}}{x_{max} - x_{min}} \\] (maps to [0,1])\n",
    "\n",
    "✅ Pros:\n",
    "- preserves original shape of distribution\n",
    "- good for bounded features and neural nets\n",
    "\n",
    "❌ Cons:\n",
    "- very sensitive to outliers (they set min/max)\n",
    "- if new data has values beyond training min/max, scaling can go outside [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16952b25",
   "metadata": {},
   "source": [
    "## Q53) What is the purpose of unit vector scaling?\n",
    "\n",
    "**Unit vector scaling (normalization)** scales each sample (row) to have unit norm:\n",
    "\\[ x' = \\frac{x}{\\|x\\|} \\]\n",
    "\n",
    "Purpose:\n",
    "- useful when direction matters more than magnitude\n",
    "- common in text features (TF-IDF), cosine similarity, nearest-neighbor retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745da7f",
   "metadata": {},
   "source": [
    "## Q54) Define Principle Component Analysis (PCA).\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique that transforms features into a new set of orthogonal axes (principal components) that capture maximum variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746fbd4",
   "metadata": {},
   "source": [
    "## Q55) Explain the steps involved in PCA.\n",
    "\n",
    "Steps in PCA:\n",
    "1. standardize data (often required)\n",
    "2. compute covariance matrix (or SVD directly)\n",
    "3. compute eigenvalues/eigenvectors (or singular vectors)\n",
    "4. sort components by eigenvalues (variance explained)\n",
    "5. choose top k components\n",
    "6. project data onto these components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482598ad",
   "metadata": {},
   "source": [
    "## Q56) Discuss the significance of eigenvalues and eigenvectors in PCA.\n",
    "\n",
    "- **Eigenvectors** define the direction of principal components (new axes).\n",
    "- **Eigenvalues** tell how much variance is captured along each eigenvector.\n",
    "Bigger eigenvalue ⇒ more important component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea602b3",
   "metadata": {},
   "source": [
    "## Q57) How does PCA help in dimensionality reduction?\n",
    "\n",
    "PCA reduces dimensionality by:\n",
    "- projecting data onto a smaller number of components that capture most variance\n",
    "Benefits:\n",
    "- less overfitting (fewer features)\n",
    "- faster training/inference\n",
    "- noise reduction\n",
    "- visualization in 2D/3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12550407",
   "metadata": {},
   "source": [
    "## Q58) Define data encoding and its importance in machine learning.\n",
    "\n",
    "**Data encoding** converts categorical variables to numeric form so ML algorithms can use them.\n",
    "Importance:\n",
    "- most ML models require numeric input\n",
    "- correct encoding prevents introducing false order relationships and improves performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6da1b2",
   "metadata": {},
   "source": [
    "## Q59) Explain Nominal Encoding and provide an example.\n",
    "\n",
    "**Nominal encoding** applies to categories with **no natural order** (e.g., color).\n",
    "Common approach: one-hot encoding.\n",
    "\n",
    "Example:\n",
    "- Color ∈ {Red, Blue, Green}\n",
    "- One-hot → Red=[1,0,0], Blue=[0,1,0], Green=[0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc3186",
   "metadata": {},
   "source": [
    "## Q60) Discuss the process of One Hot Encoding.\n",
    "\n",
    "**One-Hot Encoding** creates binary indicator columns for each category.\n",
    "Steps:\n",
    "1. find unique categories in a feature\n",
    "2. create one column per category\n",
    "3. put 1 where the row has that category, else 0\n",
    "Often drop one column to avoid dummy variable trap in linear regression (multicollinearity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462cfe2",
   "metadata": {},
   "source": [
    "## Q61) How do you handle multiple categories in One Hot Encoding?\n",
    "\n",
    "Handling many categories:\n",
    "- **Top-K + 'Other'**: keep most frequent K categories, rest → Other\n",
    "- **Hashing trick** (feature hashing)\n",
    "- **Target/mean encoding** (careful with leakage)\n",
    "- **Embeddings** (deep learning)\n",
    "- Reduce cardinality via grouping (domain knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d366e",
   "metadata": {},
   "source": [
    "## Q62) Explain Mean Encoding and its advantages.\n",
    "\n",
    "**Mean (Target) Encoding** replaces each category with the mean of the target for that category.\n",
    "Advantages:\n",
    "- handles high-cardinality categories with a single numeric column\n",
    "- often improves performance in tree/linear models\n",
    "Risks:\n",
    "- leakage and overfitting if computed on full dataset\n",
    "Best practice:\n",
    "- compute using CV folds, add smoothing, and apply only using training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed807bf5",
   "metadata": {},
   "source": [
    "## Q63) Provide examples of Ordinal Encoding and Label Encoding.\n",
    "\n",
    "- **Ordinal encoding:** categories mapped to integers in natural order.\n",
    "  Example: size {Small, Medium, Large} → {0,1,2}\n",
    "- **Label encoding:** assigns integer IDs to categories (often for target labels in classification).\n",
    "  Example: {cat, dog, fish} → {0,1,2}\n",
    "Caution: label encoding for input features can create false ordering unless the feature is truly ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d58073",
   "metadata": {},
   "source": [
    "## Q64) What is Target Guided Ordinal Encoding and how is it used?\n",
    "\n",
    "**Target Guided Ordinal Encoding** assigns order based on target statistics.\n",
    "Example:\n",
    "- categories are ordered by mean target value and then encoded 0,1,2...\n",
    "Useful when categories have meaningful relationship to target.\n",
    "Must be done carefully with cross-validation to prevent leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1a085",
   "metadata": {},
   "source": [
    "## Q65) Define covariance and its significance in statistics.\n",
    "\n",
    "**Covariance** measures how two variables vary together:\n",
    "- positive covariance: both increase together\n",
    "- negative covariance: one increases while the other decreases\n",
    "Magnitude depends on units, so covariance is not normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c84c5f",
   "metadata": {},
   "source": [
    "## Q66) Explain the process of correlation check.\n",
    "\n",
    "Correlation check process:\n",
    "1. select numeric features\n",
    "2. compute correlation matrix (Pearson for linear, Spearman for monotonic)\n",
    "3. visualize via heatmap (or print matrix)\n",
    "4. detect multicollinearity (very high correlations)\n",
    "5. decide: drop/merge features, use PCA, or regularize model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad18e3",
   "metadata": {},
   "source": [
    "## Q67) What is the Pearson Correlation Coefficient?\n",
    "\n",
    "**Pearson correlation coefficient (r)** measures linear relationship between two variables:\n",
    "- ranges from -1 to +1\n",
    "- +1 perfect positive linear\n",
    "- -1 perfect negative linear\n",
    "- 0 no linear relationship (may still have nonlinear relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32a39e",
   "metadata": {},
   "source": [
    "## Q68) How does Spearman's Rank Correlation differ from Pearson's Correlation?\n",
    "\n",
    "**Spearman correlation**:\n",
    "- based on **ranks** (monotonic relationship)\n",
    "- less sensitive to outliers and non-normal distributions\n",
    "**Pearson**:\n",
    "- uses raw values (linear relationship)\n",
    "- more sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079bc56",
   "metadata": {},
   "source": [
    "## Q69) Discuss the importance of Variance Inflation Factor (VIF) in feature selection.\n",
    "\n",
    "**VIF (Variance Inflation Factor)** detects multicollinearity among features.\n",
    "- High VIF means a feature is highly explained by other features (redundant).\n",
    "Why important:\n",
    "- multicollinearity inflates variance of coefficient estimates in linear models\n",
    "- makes interpretation unstable (coefficients can flip signs)\n",
    "Common thresholds: VIF > 5 or 10 indicates problematic multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b69cb2",
   "metadata": {},
   "source": [
    "## Q70) Define feature selection and its purpose.\n",
    "\n",
    "**Feature selection** is choosing a subset of relevant features for training.\n",
    "Purpose:\n",
    "- reduce overfitting\n",
    "- improve generalization\n",
    "- reduce training time\n",
    "- improve interpretability\n",
    "- remove redundant/noisy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067448",
   "metadata": {},
   "source": [
    "## Q71) Explain the process of Recursive Feature Elimination.\n",
    "\n",
    "**Recursive Feature Elimination (RFE)**:\n",
    "1. train a model\n",
    "2. rank features by importance (weights/coefs)\n",
    "3. remove the least important features\n",
    "4. repeat until desired number of features remains\n",
    "Often combined with cross-validation (RFECV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babaf0dd",
   "metadata": {},
   "source": [
    "## Q72) How does Backward Elimination work?\n",
    "\n",
    "**Backward elimination** (wrapper method):\n",
    "1. start with all features\n",
    "2. train model (often linear regression)\n",
    "3. remove the least significant feature (highest p-value) or smallest improvement\n",
    "4. repeat until all remaining features are significant or performance stops improving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b2bce",
   "metadata": {},
   "source": [
    "## Q73) Discuss the advantages and limitations of Forward Elimination.\n",
    "\n",
    "**Forward selection**:\n",
    "- start with zero features\n",
    "- add the feature that improves performance most at each step\n",
    "\n",
    "✅ Advantages:\n",
    "- cheaper than testing all subsets\n",
    "- can work well when few features matter\n",
    "\n",
    "❌ Limitations:\n",
    "- greedy (may miss optimal subset)\n",
    "- can be slow when many features\n",
    "- results depend on evaluation metric and split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd6f36d",
   "metadata": {},
   "source": [
    "## Q74) What is feature engineering and why is it important?\n",
    "\n",
    "**Feature engineering** is creating/transforming features to make patterns easier for a model to learn.\n",
    "Why important:\n",
    "- boosts performance even with simple models\n",
    "- captures domain knowledge\n",
    "- can reduce noise and improve generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df997152",
   "metadata": {},
   "source": [
    "## Q75) Discuss the steps involved in feature engineering.\n",
    "\n",
    "Steps in feature engineering:\n",
    "1. understand data + domain + target\n",
    "2. clean data (missing, outliers)\n",
    "3. transform variables (log, scaling)\n",
    "4. encode categoricals\n",
    "5. create interactions / aggregates / time features\n",
    "6. validate with cross-validation\n",
    "7. monitor for leakage and stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5edcb",
   "metadata": {},
   "source": [
    "## Q76) Provide examples of feature engineering techniques.\n",
    "\n",
    "Examples:\n",
    "- log transform for skewed variables\n",
    "- binning (age groups)\n",
    "- interaction features (A*B, ratios)\n",
    "- time-based features (day of week, rolling mean)\n",
    "- text features (TF-IDF)\n",
    "- polynomial features\n",
    "- group aggregates (mean per user/category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f6f8a",
   "metadata": {},
   "source": [
    "## Q77) How does feature selection differ from feature engineering?\n",
    "\n",
    "Difference:\n",
    "- **Feature engineering:** create or transform features (new information).\n",
    "- **Feature selection:** choose which features to keep (reduce dimensionality).\n",
    "Often used together: engineer useful features, then select the best subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0d431",
   "metadata": {},
   "source": [
    "## Q78) Explain the importance of feature selection in machine learning pipelines.\n",
    "\n",
    "Importance of feature selection in pipelines:\n",
    "- reduces dimensionality and computation\n",
    "- improves generalization by removing noise\n",
    "- helps interpretability for stakeholders\n",
    "- prevents multicollinearity issues in linear models\n",
    "- can improve stability and deployment speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e7851",
   "metadata": {},
   "source": [
    "## Q79) Discuss the impact of feature selection on model performance.\n",
    "\n",
    "Impact on performance:\n",
    "- can increase accuracy by removing irrelevant features\n",
    "- reduces overfitting risk\n",
    "- may decrease performance if important features are removed incorrectly\n",
    "Therefore, selection should be validated with cross-validation and stable metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061953f",
   "metadata": {},
   "source": [
    "## Q80) How do you determine which features to include in a machine-learning model?\n",
    "\n",
    "How to decide which features to include:\n",
    "- start with domain knowledge + data understanding\n",
    "- remove leakage features (future info)\n",
    "- handle missing/outliers and encode categoricals properly\n",
    "- check correlation and VIF (multicollinearity)\n",
    "- use feature importance (tree models), coefficients (linear models), SHAP\n",
    "- perform selection with CV (RFE, embedded methods)\n",
    "- compare models with/without features using validation metrics\n",
    "- keep features that improve performance and are stable/available at inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39118b76",
   "metadata": {},
   "source": [
    "---\n",
    "# Optional Demo Code (supports multiple questions)\n",
    "These code cells demonstrate key ML preprocessing and evaluation concepts mentioned in the answers.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648afffd",
   "metadata": {},
   "source": [
    "## Demo 1) Train–Validation–Test Split (supports Q14–Q18)\n",
    "\n",
    "We split data in three parts:\n",
    "- training for fitting\n",
    "- validation for tuning\n",
    "- test for final evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23068e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1200, 12) Val: (400, 12) Test: (400, 12)\n",
      "Class counts (train): [1099  101]\n",
      "Class counts (val)  : [367  33]\n",
      "Class counts (test) : [367  33]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: train+val vs test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    Xc, yc, test_size=0.20, stratify=yc, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: train vs val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42\n",
    ")  # 0.25 of 0.80 = 0.20 => 60/20/20 split\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "print(\"Class counts (train):\", np.bincount(y_train))\n",
    "print(\"Class counts (val)  :\", np.bincount(y_val))\n",
    "print(\"Class counts (test) :\", np.bincount(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0038ce",
   "metadata": {},
   "source": [
    "## Demo 2) Overfitting vs Underfitting (supports Q21–Q25)\n",
    "\n",
    "We train decision trees with different depths and compare train vs validation accuracy.\n",
    "- Very small depth → underfit\n",
    "- Very large depth → overfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3779e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.930833</td>\n",
       "      <td>0.9100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>0.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>0.9475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  train_acc  val_acc\n",
       "0        1.0   0.930833   0.9100\n",
       "1        2.0   0.936667   0.9250\n",
       "2        3.0   0.954167   0.9375\n",
       "3        5.0   0.971667   0.9475\n",
       "4       10.0   0.995833   0.9250\n",
       "5        NaN   1.000000   0.9225"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "depths = [1, 2, 3, 5, 10, None]\n",
    "rows = []\n",
    "\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    val_acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "    rows.append({\"max_depth\": d, \"train_acc\": train_acc, \"val_acc\": val_acc})\n",
    "\n",
    "pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8aad2b",
   "metadata": {},
   "source": [
    "## Demo 3) Missing Data Handling (supports Q26–Q29)\n",
    "\n",
    "We create missing values artificially and show:\n",
    "- mean imputation\n",
    "- median imputation\n",
    "- adding a missingness indicator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eed2140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count:\n",
      "f0    96\n",
      "f1    75\n",
      "dtype: int64\n",
      "Imputed shapes: (800, 10) (800, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "Xr_df = pd.DataFrame(Xr, columns=[f\"f{i}\" for i in range(Xr.shape[1])])\n",
    "\n",
    "# Create missingness in two columns\n",
    "mask = np.random.rand(*Xr_df[[\"f0\",\"f1\"]].shape) < 0.10\n",
    "Xr_df.loc[mask[:,0], \"f0\"] = np.nan\n",
    "Xr_df.loc[mask[:,1], \"f1\"] = np.nan\n",
    "\n",
    "print(\"Missing values count:\")\n",
    "print(Xr_df[[\"f0\",\"f1\"]].isna().sum())\n",
    "\n",
    "# Mean imputation\n",
    "imp_mean = SimpleImputer(strategy=\"mean\")\n",
    "X_imp_mean = imp_mean.fit_transform(Xr_df)\n",
    "\n",
    "# Add missing indicators\n",
    "X_ind = Xr_df.copy()\n",
    "X_ind[\"f0_missing\"] = X_ind[\"f0\"].isna().astype(int)\n",
    "X_ind[\"f1_missing\"] = X_ind[\"f1\"].isna().astype(int)\n",
    "\n",
    "X_imp_with_ind = SimpleImputer(strategy=\"median\").fit_transform(X_ind)\n",
    "\n",
    "print(\"Imputed shapes:\", X_imp_mean.shape, X_imp_with_ind.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b984d46",
   "metadata": {},
   "source": [
    "## Demo 4) Imbalanced Data + SMOTE (supports Q30–Q38)\n",
    "\n",
    "We compare class distribution before and after SMOTE.\n",
    "\n",
    "> If `imblearn` is not installed, run:\n",
    "`pip install imbalanced-learn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e47a65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE (train): Counter({0: 1099, 1: 101})\n",
      "After SMOTE (train):  Counter({0: 1099, 1: 1099})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Before SMOTE (train):\", Counter(y_train))\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE(random_state=42, k_neighbors=5)\n",
    "    X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "    print(\"After SMOTE (train): \", Counter(y_train_sm))\n",
    "except Exception as e:\n",
    "    print(\"SMOTE demo skipped:\", e)\n",
    "    print(\"Install with: pip install imbalanced-learn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf33baa",
   "metadata": {},
   "source": [
    "## Demo 5) Data Interpolation (supports Q39–Q41)\n",
    "\n",
    "We create a small time series with missing values and fill using linear and spline interpolation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08118378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    10.0\n",
       "1     NaN\n",
       "2    14.0\n",
       "3     NaN\n",
       "4     NaN\n",
       "5    25.0\n",
       "6    28.0\n",
       "7     NaN\n",
       "8    35.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear interpolation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    10.000000\n",
       "1    12.000000\n",
       "2    14.000000\n",
       "3    17.666667\n",
       "4    21.333333\n",
       "5    25.000000\n",
       "6    28.000000\n",
       "7    31.500000\n",
       "8    35.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spline interpolation (order=2) (requires scipy):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    10.000000\n",
       "1    12.176871\n",
       "2    14.000000\n",
       "3    17.928571\n",
       "4    21.059524\n",
       "5    25.000000\n",
       "6    28.000000\n",
       "7    31.472789\n",
       "8    35.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts = pd.Series([10, np.nan, 14, np.nan, np.nan, 25, 28, np.nan, 35])\n",
    "print(\"Original:\")\n",
    "display(ts)\n",
    "\n",
    "print(\"Linear interpolation:\")\n",
    "display(ts.interpolate(method=\"linear\"))\n",
    "\n",
    "print(\"Spline interpolation (order=2) (requires scipy):\")\n",
    "try:\n",
    "    display(ts.interpolate(method=\"spline\", order=2))\n",
    "except Exception as e:\n",
    "    print(\"Spline interpolation skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b627f24",
   "metadata": {},
   "source": [
    "## Demo 6) Outlier Detection (supports Q42–Q45)\n",
    "\n",
    "We detect outliers using:\n",
    "- IQR rule\n",
    "- Z-score rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff09fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR outliers:\n",
      "[-2.92135048  3.19310757 -2.70323229  8.          9.         -7.        ]\n",
      "\n",
      "Z-score outliers:\n",
      "[ 8.  9. -7.]\n"
     ]
    }
   ],
   "source": [
    "x = pd.Series(np.concatenate([np.random.normal(0, 1, 300), np.array([8, 9, -7])]))\n",
    "\n",
    "# IQR method\n",
    "q1, q3 = x.quantile(0.25), x.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "out_iqr = x[(x < lower) | (x > upper)]\n",
    "\n",
    "# Z-score method\n",
    "z = (x - x.mean()) / x.std(ddof=1)\n",
    "out_z = x[np.abs(z) > 3]\n",
    "\n",
    "print(\"IQR outliers:\")\n",
    "print(out_iqr.values)\n",
    "\n",
    "print(\"\\nZ-score outliers:\")\n",
    "print(out_z.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437077a",
   "metadata": {},
   "source": [
    "## Demo 7) Feature Scaling (supports Q49–Q53)\n",
    "\n",
    "We compare:\n",
    "- Standardization\n",
    "- MinMax scaling\n",
    "- Unit vector scaling (normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "607cc433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[[ 3.926 -2.084  0.141]\n",
      " [ 1.396 -0.562 -1.106]\n",
      " [-0.898 -0.766 -1.9  ]\n",
      " [   nan -0.123  0.264]\n",
      " [-0.032  0.641 -0.531]\n",
      " [   nan -1.388 -0.488]\n",
      " [-1.294 -0.696  2.125]\n",
      " [-0.555  0.428 -0.834]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nNormalizer does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m std \u001b[38;5;241m=\u001b[39m StandardScaler()\u001b[38;5;241m.\u001b[39mfit_transform(X_small)\n\u001b[1;32m      8\u001b[0m mm \u001b[38;5;241m=\u001b[39m MinMaxScaler()\u001b[38;5;241m.\u001b[39mfit_transform(X_small)\n\u001b[0;32m----> 9\u001b[0m uv \u001b[38;5;241m=\u001b[39m Normalizer(norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(X_small)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStandardized:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(std, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:2073\u001b[0m, in \u001b[0;36mNormalizer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only validates estimator's parameters.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \n\u001b[1;32m   2057\u001b[0m \u001b[38;5;124;03m    This method allows to: (i) validate the estimator's parameters and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;124;03m        Fitted transformer.\u001b[39;00m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2073\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1049\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1050\u001b[0m         array,\n\u001b[1;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    127\u001b[0m     X,\n\u001b[1;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    133\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nNormalizer does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "X_small = Xr[:8, :3]  # small slice for display\n",
    "print(\"Original:\")\n",
    "print(np.round(X_small, 3))\n",
    "\n",
    "std = StandardScaler().fit_transform(X_small)\n",
    "mm = MinMaxScaler().fit_transform(X_small)\n",
    "uv = Normalizer(norm=\"l2\").fit_transform(X_small)\n",
    "\n",
    "print(\"\\nStandardized:\")\n",
    "print(np.round(std, 3))\n",
    "\n",
    "print(\"\\nMinMax scaled:\")\n",
    "print(np.round(mm, 3))\n",
    "\n",
    "print(\"\\nUnit vector scaled (row-wise):\")\n",
    "print(np.round(uv, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e13d1",
   "metadata": {},
   "source": [
    "## Demo 8) PCA (supports Q54–Q57)\n",
    "\n",
    "We standardize data, run PCA, and show explained variance ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_std = StandardScaler().fit_transform(Xr)\n",
    "\n",
    "pca = PCA(n_components=5, random_state=42)\n",
    "Xp = pca.fit_transform(X_std)\n",
    "\n",
    "print(\"Explained variance ratio (first 5 PCs):\")\n",
    "print(np.round(pca.explained_variance_ratio_, 4))\n",
    "print(\"Cumulative variance:\")\n",
    "print(np.round(np.cumsum(pca.explained_variance_ratio_), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974261c4",
   "metadata": {},
   "source": [
    "## Demo 9) Encoding (supports Q58–Q64)\n",
    "\n",
    "We demonstrate:\n",
    "- One-hot encoding\n",
    "- Label encoding (for target labels)\n",
    "- Ordinal encoding\n",
    "- Mean/target encoding (simple illustration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fff1535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot feature names: ['color_blue' 'color_green' 'color_red']\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "Ordinal encoding (size):\n",
      "[0. 1. 2. 0. 2. 1.]\n",
      "\n",
      "Label encoding example: [0 1 1 2] classes: ['cat' 'dog' 'fish']\n",
      "\n",
      "Mean encoding map: {'blue': 0.0, 'green': 0.5, 'red': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>target</th>\n",
       "      <th>color_mean_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>small</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blue</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>green</td>\n",
       "      <td>large</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blue</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>red</td>\n",
       "      <td>large</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>green</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color    size  target  color_mean_enc\n",
       "0    red   small       1             1.0\n",
       "1   blue  medium       0             0.0\n",
       "2  green   large       1             0.5\n",
       "3   blue   small       0             0.0\n",
       "4    red   large       1             1.0\n",
       "5  green  medium       0             0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "df_cat = pd.DataFrame({\n",
    "    \"color\": [\"red\",\"blue\",\"green\",\"blue\",\"red\",\"green\"],\n",
    "    \"size\": [\"small\",\"medium\",\"large\",\"small\",\"large\",\"medium\"],\n",
    "    \"target\": [1, 0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "# One-hot for nominal 'color'\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "X_color_ohe = ohe.fit_transform(df_cat[[\"color\"]])\n",
    "print(\"One-hot feature names:\", ohe.get_feature_names_out([\"color\"]))\n",
    "print(X_color_ohe)\n",
    "\n",
    "# Ordinal for ordered 'size'\n",
    "ord_enc = OrdinalEncoder(categories=[[\"small\",\"medium\",\"large\"]])\n",
    "X_size_ord = ord_enc.fit_transform(df_cat[[\"size\"]])\n",
    "print(\"\\nOrdinal encoding (size):\")\n",
    "print(X_size_ord.ravel())\n",
    "\n",
    "# Label encoding (typically for y, not X)\n",
    "le = LabelEncoder()\n",
    "y_le = le.fit_transform([\"cat\",\"dog\",\"dog\",\"fish\"])\n",
    "print(\"\\nLabel encoding example:\", y_le, \"classes:\", le.classes_)\n",
    "\n",
    "# Mean/Target encoding (simple, MUST be CV-safe in real pipelines)\n",
    "mean_map = df_cat.groupby(\"color\")[\"target\"].mean().to_dict()\n",
    "df_cat[\"color_mean_enc\"] = df_cat[\"color\"].map(mean_map)\n",
    "print(\"\\nMean encoding map:\", mean_map)\n",
    "display(df_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ace23",
   "metadata": {},
   "source": [
    "## Demo 10) VIF and RFE (supports Q69–Q72)\n",
    "\n",
    "- VIF checks multicollinearity among features (commonly in linear models).\n",
    "- RFE selects features by recursively removing the least important.\n",
    "\n",
    "> If `statsmodels` is not installed: `pip install statsmodels`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a7f7772",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m      6\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(model, n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m rfe\u001b[38;5;241m.\u001b[39mfit(Xr, yr)\n\u001b[1;32m      9\u001b[0m selected \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(rfe\u001b[38;5;241m.\u001b[39msupport_)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m ranking \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39mranking_\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_selection/_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_selection/_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[0;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    315\u001b[0m     estimator,\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[1;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_base.py:578\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    574\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    576\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 578\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    579\u001b[0m     X, y, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse, y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    580\u001b[0m )\n\u001b[1;32m    582\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1264\u001b[0m     X,\n\u001b[1;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   1266\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1267\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1268\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   1269\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1270\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[1;32m   1271\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m   1272\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[1;32m   1273\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1274\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[1;32m   1275\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1277\u001b[0m )\n\u001b[1;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1281\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1049\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1050\u001b[0m         array,\n\u001b[1;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    127\u001b[0m     X,\n\u001b[1;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    133\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# RFE with linear regression on regression dataset\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "rfe.fit(Xr, yr)\n",
    "\n",
    "selected = np.where(rfe.support_)[0]\n",
    "ranking = rfe.ranking_\n",
    "print(\"Selected feature indices:\", selected)\n",
    "print(\"Ranking (1=selected):\", ranking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257afee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF demo (requires statsmodels)\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "    X_vif = pd.DataFrame(Xr[:, :6], columns=[f\"f{i}\" for i in range(6)])\n",
    "    X_vif = sm.add_constant(X_vif)\n",
    "\n",
    "    vif = []\n",
    "    for i in range(X_vif.shape[1]):\n",
    "        vif.append({\"feature\": X_vif.columns[i], \"VIF\": variance_inflation_factor(X_vif.values, i)})\n",
    "    pd.DataFrame(vif)\n",
    "except Exception as e:\n",
    "    print(\"VIF demo skipped:\", e)\n",
    "    print(\"Install with: pip install statsmodels\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
